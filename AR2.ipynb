{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LassoCV\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv(\"weatherAUS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Deep neural network concepts </b> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary <br>\n",
    "Deep neural networks are models that have more than just an input and output layer, but have 1 to many hidden layers. These hidden layers add to the additional complexity to the model where features can be picked apart more finely. RNN's which look to take into account sequences, or time series, to make more accuracte predictions by taking into account previous observations. A form of RNN is an LSTM, used in this project, which looks to fix a problem with RNN's specifically. While RNN's do use a form of \"memory\", LSTM's memory is longer hence the long in its name. In this case, with the rain dataset, it is a time series so it is well suited for this model. While these types of models are great when used appropriately, they also come with their own set of problems. Speed and computational complexity are two of the biggest challenges. As explored with previous for a fully connected neural network, the training time and complexity was simpiler and faster. However, with LSTM's the training time increases by a substantial amount. Further, the bigger datasets you work with and how you look to tune the hyperparameters will increase the training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of deep neural networks in processing complex patterns in data <br>\n",
    "Deep neural networks are able to learn the additional compelxity that may be represented by features. In this case giving higher priority to features that provide a more decisive result than those that may appear anywhere. With most data there will also be noise and instability within it. Despite this, deep neural networks are able to circumvent this. As well, despite overfitting being a potential concern for NN models, when used on appropriately complex models, they generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "columns_to_keep = ['Date', 'RainToday', 'Pressure9am', 'MaxTemp', 'Humidity3pm', 'WindGustSpeed', 'Cloud3pm', 'Rainfall', 'Sunshine', 'Pressure3pm', 'RainTomorrow']\n",
    "data = data.drop(columns=data.columns.difference(columns_to_keep))\n",
    "\n",
    "# Use literally everything\n",
    "# data = data.drop(columns=['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])\n",
    "\n",
    "# drop null values\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Parse through the dates so we have a sequence\n",
    "data['Date'] = data['Date'].apply(lambda x : datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "\n",
    "# Then drop the date column\n",
    "data = data.drop(columns=['Date'])\n",
    "\n",
    "\n",
    "# Encode these values\n",
    "data.loc[data['RainToday'] == 'Yes', 'RainToday'] = 1\n",
    "data.loc[data['RainToday'] == 'No', 'RainToday'] = 0\n",
    "\n",
    "# Encoding and Normalizing\n",
    "scale = StandardScaler()\n",
    "labelenc = LabelEncoder()\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "y = data[['RainTomorrow']]\n",
    "\n",
    "x = data.drop(['RainTomorrow'], axis=1)\n",
    "\n",
    "# Normalize x data \n",
    "xscale = scale.fit_transform(x)\n",
    "\n",
    "X = torch.FloatTensor(xscale)\n",
    "# Encode the response variable  \n",
    "y = labelenc.fit_transform(y)\n",
    "\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "\n",
    "seqence_size = 1000\n",
    "batch_size = 25\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model developement\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate=0.0):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        # Normalization layer\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        # Drop out layer\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        # Fully connected layer\n",
    "        self.lin = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # To output layer\n",
    "        self.lin2 = nn.Linear(hidden_size, output_size)\n",
    "        # Output layer\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    # Step forward\n",
    "    def forward(self, x):\n",
    "        # Lstm output\n",
    "        val, (hidden_state, cell_state) = self.lstm(x)\n",
    "\n",
    "        # Do through every other layer\n",
    "        return self.sig(self.lin2(self.relu(self.lin(self.dropout(self.norm(val))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hyperparameters before optimization\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 25\n",
    "num_layers = 8\n",
    "output_size = 1\n",
    "\n",
    "# Make the model\n",
    "clf = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "opt = optim.Adam(clf.parameters(), lr=0.001)\n",
    "# Loss function\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 100, Loss: 0.423104465007782\n",
      "Epoch: 0, Batch: 200, Loss: 0.6269589066505432\n",
      "Epoch: 0, Batch: 300, Loss: 0.4575839936733246\n",
      "Epoch: 0, Batch: 400, Loss: 0.7371435761451721\n",
      "Epoch: 0, Batch: 500, Loss: 0.7127088904380798\n",
      "Epoch: 0, Batch: 600, Loss: 0.49902573227882385\n",
      "Epoch: 0, Batch: 700, Loss: 0.6208954453468323\n",
      "Epoch: 0, Batch: 800, Loss: 0.553520143032074\n",
      "Epoch: 0, Batch: 900, Loss: 0.5547534823417664\n",
      "Epoch: 0, Batch: 1000, Loss: 0.46147289872169495\n",
      "Epoch: 0, Batch: 1100, Loss: 0.5561729669570923\n",
      "Epoch: 0, Batch: 1200, Loss: 0.5143591165542603\n",
      "Epoch: 0, Batch: 1300, Loss: 0.6061510443687439\n",
      "Epoch: 0, Batch: 1400, Loss: 0.49440228939056396\n",
      "Epoch: 0, Batch: 1500, Loss: 0.3912881016731262\n",
      "Epoch: 0, Batch: 1600, Loss: 0.5046612024307251\n",
      "Epoch [1/15], Loss: 0.6528722643852234\n",
      "Epoch: 1, Batch: 100, Loss: 0.5863404273986816\n",
      "Epoch: 1, Batch: 200, Loss: 0.631150484085083\n",
      "Epoch: 1, Batch: 300, Loss: 0.4953569173812866\n",
      "Epoch: 1, Batch: 400, Loss: 0.5602943897247314\n",
      "Epoch: 1, Batch: 500, Loss: 0.5777997374534607\n",
      "Epoch: 1, Batch: 600, Loss: 0.6021104454994202\n",
      "Epoch: 1, Batch: 700, Loss: 0.4038439095020294\n",
      "Epoch: 1, Batch: 800, Loss: 0.45016735792160034\n",
      "Epoch: 1, Batch: 900, Loss: 0.4108712077140808\n",
      "Epoch: 1, Batch: 1000, Loss: 0.4539793133735657\n",
      "Epoch: 1, Batch: 1100, Loss: 0.6088711023330688\n",
      "Epoch: 1, Batch: 1200, Loss: 0.660204291343689\n",
      "Epoch: 1, Batch: 1300, Loss: 0.4868660867214203\n",
      "Epoch: 1, Batch: 1400, Loss: 0.7520923018455505\n",
      "Epoch: 1, Batch: 1500, Loss: 0.7189416289329529\n",
      "Epoch: 1, Batch: 1600, Loss: 0.4560316503047943\n",
      "Epoch [2/15], Loss: 0.2583671808242798\n",
      "Epoch: 2, Batch: 100, Loss: 0.5039457678794861\n",
      "Epoch: 2, Batch: 200, Loss: 0.4944823384284973\n",
      "Epoch: 2, Batch: 300, Loss: 0.6279632449150085\n",
      "Epoch: 2, Batch: 400, Loss: 0.45100289583206177\n",
      "Epoch: 2, Batch: 500, Loss: 0.6086284518241882\n",
      "Epoch: 2, Batch: 600, Loss: 0.45596420764923096\n",
      "Epoch: 2, Batch: 700, Loss: 0.4552319347858429\n",
      "Epoch: 2, Batch: 800, Loss: 0.6099833250045776\n",
      "Epoch: 2, Batch: 900, Loss: 0.4957881569862366\n",
      "Epoch: 2, Batch: 1000, Loss: 0.5918249487876892\n",
      "Epoch: 2, Batch: 1100, Loss: 0.5447266101837158\n",
      "Epoch: 2, Batch: 1200, Loss: 0.5642130374908447\n",
      "Epoch: 2, Batch: 1300, Loss: 0.6019943356513977\n",
      "Epoch: 2, Batch: 1400, Loss: 0.4005414843559265\n",
      "Epoch: 2, Batch: 1500, Loss: 0.5558801889419556\n",
      "Epoch: 2, Batch: 1600, Loss: 0.6040873527526855\n",
      "Epoch [3/15], Loss: 0.750616729259491\n",
      "Epoch: 3, Batch: 100, Loss: 0.49526435136795044\n",
      "Epoch: 3, Batch: 200, Loss: 0.4104653298854828\n",
      "Epoch: 3, Batch: 300, Loss: 0.7052356004714966\n",
      "Epoch: 3, Batch: 400, Loss: 0.4495397210121155\n",
      "Epoch: 3, Batch: 500, Loss: 0.5081079006195068\n",
      "Epoch: 3, Batch: 600, Loss: 0.409996896982193\n",
      "Epoch: 3, Batch: 700, Loss: 0.5000180602073669\n",
      "Epoch: 3, Batch: 800, Loss: 0.6118494868278503\n",
      "Epoch: 3, Batch: 900, Loss: 0.6498013138771057\n",
      "Epoch: 3, Batch: 1000, Loss: 0.46037372946739197\n",
      "Epoch: 3, Batch: 1100, Loss: 0.4093495309352875\n",
      "Epoch: 3, Batch: 1200, Loss: 0.5506683588027954\n",
      "Epoch: 3, Batch: 1300, Loss: 0.5193784236907959\n",
      "Epoch: 3, Batch: 1400, Loss: 0.6061984896659851\n",
      "Epoch: 3, Batch: 1500, Loss: 0.4995039999485016\n",
      "Epoch: 3, Batch: 1600, Loss: 0.708645224571228\n",
      "Epoch [4/15], Loss: 0.6265843510627747\n",
      "Epoch: 4, Batch: 100, Loss: 0.33974751830101013\n",
      "Epoch: 4, Batch: 200, Loss: 0.6391279101371765\n",
      "Epoch: 4, Batch: 300, Loss: 0.45117947459220886\n",
      "Epoch: 4, Batch: 400, Loss: 0.4068708121776581\n",
      "Epoch: 4, Batch: 500, Loss: 0.44722068309783936\n",
      "Epoch: 4, Batch: 600, Loss: 0.5162357091903687\n",
      "Epoch: 4, Batch: 700, Loss: 0.6892107129096985\n",
      "Epoch: 4, Batch: 800, Loss: 0.31639689207077026\n",
      "Epoch: 4, Batch: 900, Loss: 0.4956066608428955\n",
      "Epoch: 4, Batch: 1000, Loss: 0.3855174779891968\n",
      "Epoch: 4, Batch: 1100, Loss: 0.33874017000198364\n",
      "Epoch: 4, Batch: 1200, Loss: 0.4481465816497803\n",
      "Epoch: 4, Batch: 1300, Loss: 0.3427368104457855\n",
      "Epoch: 4, Batch: 1400, Loss: 0.43576470017433167\n",
      "Epoch: 4, Batch: 1500, Loss: 0.3673562705516815\n",
      "Epoch: 4, Batch: 1600, Loss: 0.316150426864624\n",
      "Epoch [5/15], Loss: 0.29316264390945435\n",
      "Epoch: 5, Batch: 100, Loss: 0.2900034785270691\n",
      "Epoch: 5, Batch: 200, Loss: 0.5137271285057068\n",
      "Epoch: 5, Batch: 300, Loss: 0.5627579689025879\n",
      "Epoch: 5, Batch: 400, Loss: 0.323842316865921\n",
      "Epoch: 5, Batch: 500, Loss: 0.3555099368095398\n",
      "Epoch: 5, Batch: 600, Loss: 0.22319169342517853\n",
      "Epoch: 5, Batch: 700, Loss: 0.31431844830513\n",
      "Epoch: 5, Batch: 800, Loss: 0.5421464443206787\n",
      "Epoch: 5, Batch: 900, Loss: 0.5475916862487793\n",
      "Epoch: 5, Batch: 1000, Loss: 0.5576775074005127\n",
      "Epoch: 5, Batch: 1100, Loss: 0.5204408168792725\n",
      "Epoch: 5, Batch: 1200, Loss: 0.47372955083847046\n",
      "Epoch: 5, Batch: 1300, Loss: 0.550118088722229\n",
      "Epoch: 5, Batch: 1400, Loss: 0.36918914318084717\n",
      "Epoch: 5, Batch: 1500, Loss: 0.26497477293014526\n",
      "Epoch: 5, Batch: 1600, Loss: 0.30486372113227844\n",
      "Epoch [6/15], Loss: 0.9116389155387878\n",
      "Epoch: 6, Batch: 100, Loss: 0.2257932722568512\n",
      "Epoch: 6, Batch: 200, Loss: 0.26596391201019287\n",
      "Epoch: 6, Batch: 300, Loss: 0.45853450894355774\n",
      "Epoch: 6, Batch: 400, Loss: 0.515454113483429\n",
      "Epoch: 6, Batch: 500, Loss: 0.5349720120429993\n",
      "Epoch: 6, Batch: 600, Loss: 0.25097978115081787\n",
      "Epoch: 6, Batch: 700, Loss: 0.5404684543609619\n",
      "Epoch: 6, Batch: 800, Loss: 0.1408911943435669\n",
      "Epoch: 6, Batch: 900, Loss: 0.27150872349739075\n",
      "Epoch: 6, Batch: 1000, Loss: 0.3890499770641327\n",
      "Epoch: 6, Batch: 1100, Loss: 0.24571070075035095\n",
      "Epoch: 6, Batch: 1200, Loss: 0.2598930597305298\n",
      "Epoch: 6, Batch: 1300, Loss: 0.406620055437088\n",
      "Epoch: 6, Batch: 1400, Loss: 0.31743311882019043\n",
      "Epoch: 6, Batch: 1500, Loss: 0.40398746728897095\n",
      "Epoch: 6, Batch: 1600, Loss: 0.3556350767612457\n",
      "Epoch [7/15], Loss: 0.23930859565734863\n",
      "Epoch: 7, Batch: 100, Loss: 0.45490267872810364\n",
      "Epoch: 7, Batch: 200, Loss: 0.27193304896354675\n",
      "Epoch: 7, Batch: 300, Loss: 0.3578416407108307\n",
      "Epoch: 7, Batch: 400, Loss: 0.27940842509269714\n",
      "Epoch: 7, Batch: 500, Loss: 0.41269347071647644\n",
      "Epoch: 7, Batch: 600, Loss: 0.32612067461013794\n",
      "Epoch: 7, Batch: 700, Loss: 0.41660216450691223\n",
      "Epoch: 7, Batch: 800, Loss: 0.29738736152648926\n",
      "Epoch: 7, Batch: 900, Loss: 0.286222904920578\n",
      "Epoch: 7, Batch: 1000, Loss: 0.25715959072113037\n",
      "Epoch: 7, Batch: 1100, Loss: 0.3078031837940216\n",
      "Epoch: 7, Batch: 1200, Loss: 0.41408470273017883\n",
      "Epoch: 7, Batch: 1300, Loss: 0.5300214290618896\n",
      "Epoch: 7, Batch: 1400, Loss: 0.33617672324180603\n",
      "Epoch: 7, Batch: 1500, Loss: 0.26798829436302185\n",
      "Epoch: 7, Batch: 1600, Loss: 0.570841908454895\n",
      "Epoch [8/15], Loss: 0.3307730555534363\n",
      "Epoch: 8, Batch: 100, Loss: 0.49314767122268677\n",
      "Epoch: 8, Batch: 200, Loss: 0.45846012234687805\n",
      "Epoch: 8, Batch: 300, Loss: 0.28704142570495605\n",
      "Epoch: 8, Batch: 400, Loss: 0.2887355089187622\n",
      "Epoch: 8, Batch: 500, Loss: 0.29868483543395996\n",
      "Epoch: 8, Batch: 600, Loss: 0.4664193093776703\n",
      "Epoch: 8, Batch: 700, Loss: 0.32269054651260376\n",
      "Epoch: 8, Batch: 800, Loss: 0.6466361880302429\n",
      "Epoch: 8, Batch: 900, Loss: 0.12890534102916718\n",
      "Epoch: 8, Batch: 1000, Loss: 0.46477627754211426\n",
      "Epoch: 8, Batch: 1100, Loss: 0.21006794273853302\n",
      "Epoch: 8, Batch: 1200, Loss: 0.2840433716773987\n",
      "Epoch: 8, Batch: 1300, Loss: 0.3339768946170807\n",
      "Epoch: 8, Batch: 1400, Loss: 0.40336981415748596\n",
      "Epoch: 8, Batch: 1500, Loss: 0.4229252338409424\n",
      "Epoch: 8, Batch: 1600, Loss: 0.35023003816604614\n",
      "Epoch [9/15], Loss: 0.5198105573654175\n",
      "Epoch: 9, Batch: 100, Loss: 0.4419301152229309\n",
      "Epoch: 9, Batch: 200, Loss: 0.47735103964805603\n",
      "Epoch: 9, Batch: 300, Loss: 0.36467206478118896\n",
      "Epoch: 9, Batch: 400, Loss: 0.2813533544540405\n",
      "Epoch: 9, Batch: 500, Loss: 0.5715469121932983\n",
      "Epoch: 9, Batch: 600, Loss: 0.6049543023109436\n",
      "Epoch: 9, Batch: 700, Loss: 0.3162733018398285\n",
      "Epoch: 9, Batch: 800, Loss: 0.48726996779441833\n",
      "Epoch: 9, Batch: 900, Loss: 0.48897475004196167\n",
      "Epoch: 9, Batch: 1000, Loss: 0.2948211431503296\n",
      "Epoch: 9, Batch: 1100, Loss: 0.45680832862854004\n",
      "Epoch: 9, Batch: 1200, Loss: 0.4216116964817047\n",
      "Epoch: 9, Batch: 1300, Loss: 0.20889897644519806\n",
      "Epoch: 9, Batch: 1400, Loss: 0.11936356872320175\n",
      "Epoch: 9, Batch: 1500, Loss: 0.3636897802352905\n",
      "Epoch: 9, Batch: 1600, Loss: 0.3122093975543976\n",
      "Epoch [10/15], Loss: 0.24268491566181183\n",
      "Epoch: 10, Batch: 100, Loss: 0.3560112714767456\n",
      "Epoch: 10, Batch: 200, Loss: 0.37621966004371643\n",
      "Epoch: 10, Batch: 300, Loss: 0.21055079996585846\n",
      "Epoch: 10, Batch: 400, Loss: 0.32917481660842896\n",
      "Epoch: 10, Batch: 500, Loss: 0.32072314620018005\n",
      "Epoch: 10, Batch: 600, Loss: 0.2955659329891205\n",
      "Epoch: 10, Batch: 700, Loss: 0.4462410807609558\n",
      "Epoch: 10, Batch: 800, Loss: 0.2610044777393341\n",
      "Epoch: 10, Batch: 900, Loss: 0.23285770416259766\n",
      "Epoch: 10, Batch: 1000, Loss: 0.3480130732059479\n",
      "Epoch: 10, Batch: 1100, Loss: 0.2582826018333435\n",
      "Epoch: 10, Batch: 1200, Loss: 0.5434216856956482\n",
      "Epoch: 10, Batch: 1300, Loss: 0.123232401907444\n",
      "Epoch: 10, Batch: 1400, Loss: 0.4624903202056885\n",
      "Epoch: 10, Batch: 1500, Loss: 0.5178515911102295\n",
      "Epoch: 10, Batch: 1600, Loss: 0.25034716725349426\n",
      "Epoch [11/15], Loss: 0.18897327780723572\n",
      "Epoch: 11, Batch: 100, Loss: 0.4342557191848755\n",
      "Epoch: 11, Batch: 200, Loss: 0.29252615571022034\n",
      "Epoch: 11, Batch: 300, Loss: 0.5169572234153748\n",
      "Epoch: 11, Batch: 400, Loss: 0.2958085536956787\n",
      "Epoch: 11, Batch: 500, Loss: 0.3371339440345764\n",
      "Epoch: 11, Batch: 600, Loss: 0.22809241712093353\n",
      "Epoch: 11, Batch: 700, Loss: 0.34668442606925964\n",
      "Epoch: 11, Batch: 800, Loss: 0.24840250611305237\n",
      "Epoch: 11, Batch: 900, Loss: 0.31141427159309387\n",
      "Epoch: 11, Batch: 1000, Loss: 0.3856238126754761\n",
      "Epoch: 11, Batch: 1100, Loss: 0.3049839735031128\n",
      "Epoch: 11, Batch: 1200, Loss: 0.1864604353904724\n",
      "Epoch: 11, Batch: 1300, Loss: 0.12842848896980286\n",
      "Epoch: 11, Batch: 1400, Loss: 0.3313775360584259\n",
      "Epoch: 11, Batch: 1500, Loss: 0.2910703420639038\n",
      "Epoch: 11, Batch: 1600, Loss: 0.5368131399154663\n",
      "Epoch [12/15], Loss: 0.49901771545410156\n",
      "Epoch: 12, Batch: 100, Loss: 0.33060547709465027\n",
      "Epoch: 12, Batch: 200, Loss: 0.2918053865432739\n",
      "Epoch: 12, Batch: 300, Loss: 0.3806430399417877\n",
      "Epoch: 12, Batch: 400, Loss: 0.25658026337623596\n",
      "Epoch: 12, Batch: 500, Loss: 0.4477253258228302\n",
      "Epoch: 12, Batch: 600, Loss: 0.3463098406791687\n",
      "Epoch: 12, Batch: 700, Loss: 0.22979934513568878\n",
      "Epoch: 12, Batch: 800, Loss: 0.2935401499271393\n",
      "Epoch: 12, Batch: 900, Loss: 0.24190036952495575\n",
      "Epoch: 12, Batch: 1000, Loss: 0.2801278531551361\n",
      "Epoch: 12, Batch: 1100, Loss: 0.28900447487831116\n",
      "Epoch: 12, Batch: 1200, Loss: 0.2709323763847351\n",
      "Epoch: 12, Batch: 1300, Loss: 0.5375430583953857\n",
      "Epoch: 12, Batch: 1400, Loss: 0.29731106758117676\n",
      "Epoch: 12, Batch: 1500, Loss: 0.31610316038131714\n",
      "Epoch: 12, Batch: 1600, Loss: 0.26732099056243896\n",
      "Epoch [13/15], Loss: 0.3804308772087097\n",
      "Epoch: 13, Batch: 100, Loss: 0.32956254482269287\n",
      "Epoch: 13, Batch: 200, Loss: 0.263010710477829\n",
      "Epoch: 13, Batch: 300, Loss: 0.2772679030895233\n",
      "Epoch: 13, Batch: 400, Loss: 0.41402846574783325\n",
      "Epoch: 13, Batch: 500, Loss: 0.36495792865753174\n",
      "Epoch: 13, Batch: 600, Loss: 0.27460113167762756\n",
      "Epoch: 13, Batch: 700, Loss: 0.2589847445487976\n",
      "Epoch: 13, Batch: 800, Loss: 0.08869507908821106\n",
      "Epoch: 13, Batch: 900, Loss: 0.21805183589458466\n",
      "Epoch: 13, Batch: 1000, Loss: 0.2990420460700989\n",
      "Epoch: 13, Batch: 1100, Loss: 0.41239434480667114\n",
      "Epoch: 13, Batch: 1200, Loss: 0.25155532360076904\n",
      "Epoch: 13, Batch: 1300, Loss: 0.2945629060268402\n",
      "Epoch: 13, Batch: 1400, Loss: 0.28857430815696716\n",
      "Epoch: 13, Batch: 1500, Loss: 0.26886579394340515\n",
      "Epoch: 13, Batch: 1600, Loss: 0.432503879070282\n",
      "Epoch [14/15], Loss: 0.2852204740047455\n",
      "Epoch: 14, Batch: 100, Loss: 0.2758790850639343\n",
      "Epoch: 14, Batch: 200, Loss: 0.3275156319141388\n",
      "Epoch: 14, Batch: 300, Loss: 0.30548393726348877\n",
      "Epoch: 14, Batch: 400, Loss: 0.29942822456359863\n",
      "Epoch: 14, Batch: 500, Loss: 0.4275769889354706\n",
      "Epoch: 14, Batch: 600, Loss: 0.34152817726135254\n",
      "Epoch: 14, Batch: 700, Loss: 0.2961106598377228\n",
      "Epoch: 14, Batch: 800, Loss: 0.3108712434768677\n",
      "Epoch: 14, Batch: 900, Loss: 0.3197038471698761\n",
      "Epoch: 14, Batch: 1000, Loss: 0.24220070242881775\n",
      "Epoch: 14, Batch: 1100, Loss: 0.5314599275588989\n",
      "Epoch: 14, Batch: 1200, Loss: 0.1783783882856369\n",
      "Epoch: 14, Batch: 1300, Loss: 0.3534877300262451\n",
      "Epoch: 14, Batch: 1400, Loss: 0.519409716129303\n",
      "Epoch: 14, Batch: 1500, Loss: 0.26641812920570374\n",
      "Epoch: 14, Batch: 1600, Loss: 0.16315366327762604\n",
      "Epoch [15/15], Loss: 0.2732279300689697\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "for epoch in range(15):\n",
    "    batch = 0\n",
    "    # Do in batches\n",
    "    for data, label in train_loader:\n",
    "        \n",
    "        # Get predictions\n",
    "        yhat = clf(data).squeeze(dim=1)\n",
    "\n",
    "        # Find loss\n",
    "        loss = loss_fn(yhat, label.float())\n",
    "\n",
    "        # Back propagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Check every 100th batch\n",
    "        batch += 1\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch}, Loss: {loss}')\n",
    "    # Find epoch performance\n",
    "    print(f'Epoch [{epoch + 1}/{15}], Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't even know if I use this\n",
    "def printStats(predicitons, reals):\n",
    "    # Get the total amount of predictions\n",
    "    total_samples = len(reals)\n",
    "\n",
    "    # convert the predictions of the model to classifications\n",
    "    accuracy = predicitons.ge(0.5)\n",
    "    # Reshape the array to get the accuracy\n",
    "    accuracy = accuracy.int().reshape(1, -1)\n",
    "\n",
    "    # Format the test set for the comparison\n",
    "    y_test_tensor = torch.tensor(reals)\n",
    "\n",
    "    # Print the accuracy\n",
    "    # See the accuracy\n",
    "    print(f'Accuracy: {((accuracy == y_test_tensor).sum() / total_samples).item()}')\n",
    "    # print(((accuracy == y_test_tensor).sum() / total_samples).item().dtype)\n",
    "    y_test_tensor = y_test_tensor.reshape(1, -1)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test_tensor.view(-1).numpy(), accuracy.view(-1).numpy()))\n",
    "    return (((accuracy == y_test_tensor).sum() / total_samples).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8540933098591549\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "y_test_tens = torch.LongTensor(y_test)\n",
    "test_dataset = TensorDataset(x_test, y_test_tens)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Don't train the model any more\n",
    "clf.eval()\n",
    "\n",
    "total_samples = 0\n",
    "correct_predictions = 0\n",
    "for batch in test_loader:\n",
    "    out = clf(batch[0]).squeeze(dim=1)\n",
    "\n",
    "    # Get output\n",
    "    out = out.ge(0.5)\n",
    "    out = out.int().reshape(1, -1)\n",
    "\n",
    "    # Update counts\n",
    "    correct_predictions += torch.sum(out == batch[1]).item()\n",
    "    total_samples += batch[1].size(0)\n",
    "\n",
    "# Print accuracy\n",
    "print(f'Accuracy: {correct_predictions / total_samples}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.8808826208114624\n",
      "Epoch [2/15], Loss: 0.8923761248588562\n",
      "Epoch [3/15], Loss: 0.8928030133247375\n",
      "Epoch [4/15], Loss: 0.8245386481285095\n",
      "Epoch [5/15], Loss: 0.941407322883606\n",
      "Epoch [6/15], Loss: 0.858975350856781\n",
      "Epoch [7/15], Loss: 0.9207064509391785\n",
      "Epoch [8/15], Loss: 0.8217532634735107\n",
      "Epoch [9/15], Loss: 0.8567960858345032\n",
      "Epoch [10/15], Loss: 0.9605319499969482\n",
      "Epoch [11/15], Loss: 0.9338351488113403\n",
      "Epoch [12/15], Loss: 0.8633711934089661\n",
      "Epoch [13/15], Loss: 0.861169695854187\n",
      "Epoch [14/15], Loss: 0.9762448668479919\n",
      "Epoch [15/15], Loss: 0.8563021421432495\n",
      "Accuracy: 0.2250770246478873\n",
      "Model 1 finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.6779001951217651\n",
      "Epoch [2/15], Loss: 0.65106600522995\n",
      "Epoch [3/15], Loss: 0.6364338994026184\n",
      "Epoch [4/15], Loss: 0.6740342378616333\n",
      "Epoch [5/15], Loss: 0.6651738286018372\n",
      "Epoch [6/15], Loss: 0.6524777412414551\n",
      "Epoch [7/15], Loss: 0.6514087915420532\n",
      "Epoch [8/15], Loss: 0.6508761048316956\n",
      "Epoch [9/15], Loss: 0.6367076635360718\n",
      "Epoch [10/15], Loss: 0.6313464045524597\n",
      "Epoch [11/15], Loss: 0.6329087615013123\n",
      "Epoch [12/15], Loss: 0.6354649662971497\n",
      "Epoch [13/15], Loss: 0.658370316028595\n",
      "Epoch [14/15], Loss: 0.6922593116760254\n",
      "Epoch [15/15], Loss: 0.626977801322937\n",
      "Accuracy: 0.750825264084507\n",
      "Model 2 finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.7727003693580627\n",
      "Epoch [2/15], Loss: 0.7643250226974487\n",
      "Epoch [3/15], Loss: 0.7338169813156128\n",
      "Epoch [4/15], Loss: 0.7783766388893127\n",
      "Epoch [5/15], Loss: 0.7988974452018738\n",
      "Epoch [6/15], Loss: 0.7806248664855957\n",
      "Epoch [7/15], Loss: 0.7698679566383362\n",
      "Epoch [8/15], Loss: 0.755848228931427\n",
      "Epoch [9/15], Loss: 0.7614108324050903\n",
      "Epoch [10/15], Loss: 0.7418099045753479\n",
      "Epoch [11/15], Loss: 0.7568839192390442\n",
      "Epoch [12/15], Loss: 0.7757710218429565\n",
      "Epoch [13/15], Loss: 0.7478230595588684\n",
      "Epoch [14/15], Loss: 0.785753071308136\n",
      "Epoch [15/15], Loss: 0.7834633588790894\n",
      "Accuracy: 0.26700044014084506\n",
      "Model 3 finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.6812231540679932\n",
      "Epoch [2/15], Loss: 0.6711939573287964\n",
      "Epoch [3/15], Loss: 0.6881669163703918\n",
      "Epoch [4/15], Loss: 0.6827751398086548\n",
      "Epoch [5/15], Loss: 0.7136669158935547\n",
      "Epoch [6/15], Loss: 0.6859149932861328\n",
      "Epoch [7/15], Loss: 0.7038635015487671\n",
      "Epoch [8/15], Loss: 0.7221837043762207\n",
      "Epoch [9/15], Loss: 0.6788902282714844\n",
      "Epoch [10/15], Loss: 0.6866686940193176\n",
      "Epoch [11/15], Loss: 0.6667006611824036\n",
      "Epoch [12/15], Loss: 0.6888329982757568\n",
      "Epoch [13/15], Loss: 0.7007710933685303\n",
      "Epoch [14/15], Loss: 0.6774211525917053\n",
      "Epoch [15/15], Loss: 0.6913421154022217\n",
      "Accuracy: 0.5255281690140845\n",
      "Model 4 finished\n",
      "Epoch [1/15], Loss: 0.6196545362472534\n",
      "Epoch [2/15], Loss: 0.550919234752655\n",
      "Epoch [3/15], Loss: 0.5167945623397827\n",
      "Epoch [4/15], Loss: 0.5705490112304688\n",
      "Epoch [5/15], Loss: 0.5219389200210571\n",
      "Epoch [6/15], Loss: 0.4949495196342468\n",
      "Epoch [7/15], Loss: 0.5625052452087402\n",
      "Epoch [8/15], Loss: 0.5622072219848633\n",
      "Epoch [9/15], Loss: 0.45089563727378845\n",
      "Epoch [10/15], Loss: 0.5168307423591614\n",
      "Epoch [11/15], Loss: 0.5190384387969971\n",
      "Epoch [12/15], Loss: 0.5480408668518066\n",
      "Epoch [13/15], Loss: 0.5481752157211304\n",
      "Epoch [14/15], Loss: 0.5190499424934387\n",
      "Epoch [15/15], Loss: 0.5452360510826111\n",
      "Accuracy: 0.7849911971830986\n",
      "Model 5 finished\n",
      "Epoch [1/15], Loss: 0.802701473236084\n",
      "Epoch [2/15], Loss: 0.8182428479194641\n",
      "Epoch [3/15], Loss: 0.820999264717102\n",
      "Epoch [4/15], Loss: 0.7932341694831848\n",
      "Epoch [5/15], Loss: 0.821905255317688\n",
      "Epoch [6/15], Loss: 0.8007727265357971\n",
      "Epoch [7/15], Loss: 0.8434540629386902\n",
      "Epoch [8/15], Loss: 0.8242679834365845\n",
      "Epoch [9/15], Loss: 0.7784989476203918\n",
      "Epoch [10/15], Loss: 0.8554213643074036\n",
      "Epoch [11/15], Loss: 0.808872401714325\n",
      "Epoch [12/15], Loss: 0.7894514203071594\n",
      "Epoch [13/15], Loss: 0.871031641960144\n",
      "Epoch [14/15], Loss: 0.8241681456565857\n",
      "Epoch [15/15], Loss: 0.7800235748291016\n",
      "Accuracy: 0.22975352112676056\n",
      "Model 6 finished\n",
      "Epoch [1/15], Loss: 0.8348969221115112\n",
      "Epoch [2/15], Loss: 0.7871190309524536\n",
      "Epoch [3/15], Loss: 0.7883893251419067\n",
      "Epoch [4/15], Loss: 0.7863200306892395\n",
      "Epoch [5/15], Loss: 0.7857673168182373\n",
      "Epoch [6/15], Loss: 0.7924279570579529\n",
      "Epoch [7/15], Loss: 0.7621390223503113\n",
      "Epoch [8/15], Loss: 0.815048098564148\n",
      "Epoch [9/15], Loss: 0.7540906667709351\n",
      "Epoch [10/15], Loss: 0.7769440412521362\n",
      "Epoch [11/15], Loss: 0.8009910583496094\n",
      "Epoch [12/15], Loss: 0.7801073789596558\n",
      "Epoch [13/15], Loss: 0.7598716020584106\n",
      "Epoch [14/15], Loss: 0.7809268236160278\n",
      "Epoch [15/15], Loss: 0.7477812767028809\n",
      "Accuracy: 0.2322293133802817\n",
      "Model 7 finished\n",
      "Epoch [1/15], Loss: 0.697701096534729\n",
      "Epoch [2/15], Loss: 0.7415879964828491\n",
      "Epoch [3/15], Loss: 0.7054328918457031\n",
      "Epoch [4/15], Loss: 0.6856619715690613\n",
      "Epoch [5/15], Loss: 0.6989651322364807\n",
      "Epoch [6/15], Loss: 0.7246072292327881\n",
      "Epoch [7/15], Loss: 0.7016282081604004\n",
      "Epoch [8/15], Loss: 0.7338694334030151\n",
      "Epoch [9/15], Loss: 0.7016260027885437\n",
      "Epoch [10/15], Loss: 0.7114325165748596\n",
      "Epoch [11/15], Loss: 0.7141327261924744\n",
      "Epoch [12/15], Loss: 0.7061032652854919\n",
      "Epoch [13/15], Loss: 0.7174320220947266\n",
      "Epoch [14/15], Loss: 0.709778904914856\n",
      "Epoch [15/15], Loss: 0.6835138201713562\n",
      "Accuracy: 0.3337918133802817\n",
      "Model 8 finished\n",
      "Epoch [1/15], Loss: 0.6606507301330566\n",
      "Epoch [2/15], Loss: 0.6778731942176819\n",
      "Epoch [3/15], Loss: 0.64801424741745\n",
      "Epoch [4/15], Loss: 0.7048510313034058\n",
      "Epoch [5/15], Loss: 0.7031205892562866\n",
      "Epoch [6/15], Loss: 0.6596060991287231\n",
      "Epoch [7/15], Loss: 0.6676760911941528\n",
      "Epoch [8/15], Loss: 0.6852346658706665\n",
      "Epoch [9/15], Loss: 0.6687154173851013\n",
      "Epoch [10/15], Loss: 0.664546549320221\n",
      "Epoch [11/15], Loss: 0.6454004645347595\n",
      "Epoch [12/15], Loss: 0.6706777811050415\n",
      "Epoch [13/15], Loss: 0.6708539128303528\n",
      "Epoch [14/15], Loss: 0.7016205787658691\n",
      "Epoch [15/15], Loss: 0.6625598669052124\n",
      "Accuracy: 0.6245048415492958\n",
      "Model 9 finished\n",
      "Epoch [1/15], Loss: 0.7170069217681885\n",
      "Epoch [2/15], Loss: 0.7294028997421265\n",
      "Epoch [3/15], Loss: 0.7358893752098083\n",
      "Epoch [4/15], Loss: 0.7337619066238403\n",
      "Epoch [5/15], Loss: 0.7782736420631409\n",
      "Epoch [6/15], Loss: 0.750365674495697\n",
      "Epoch [7/15], Loss: 0.7492100596427917\n",
      "Epoch [8/15], Loss: 0.7451388239860535\n",
      "Epoch [9/15], Loss: 0.8040327429771423\n",
      "Epoch [10/15], Loss: 0.7886631488800049\n",
      "Epoch [11/15], Loss: 0.7301421165466309\n",
      "Epoch [12/15], Loss: 0.7370548248291016\n",
      "Epoch [13/15], Loss: 0.7209327220916748\n",
      "Epoch [14/15], Loss: 0.7863108515739441\n",
      "Epoch [15/15], Loss: 0.7555970549583435\n",
      "Accuracy: 0.39200044014084506\n",
      "Model 10 finished\n",
      "Epoch [1/15], Loss: 0.7049326300621033\n",
      "Epoch [2/15], Loss: 0.7095811367034912\n",
      "Epoch [3/15], Loss: 0.6754679679870605\n",
      "Epoch [4/15], Loss: 0.7015830874443054\n",
      "Epoch [5/15], Loss: 0.6843770742416382\n",
      "Epoch [6/15], Loss: 0.6914893388748169\n",
      "Epoch [7/15], Loss: 0.7162694334983826\n",
      "Epoch [8/15], Loss: 0.7110541462898254\n",
      "Epoch [9/15], Loss: 0.7124201655387878\n",
      "Epoch [10/15], Loss: 0.669884979724884\n",
      "Epoch [11/15], Loss: 0.6986653208732605\n",
      "Epoch [12/15], Loss: 0.6946635842323303\n",
      "Epoch [13/15], Loss: 0.6733109354972839\n",
      "Epoch [14/15], Loss: 0.6898260712623596\n",
      "Epoch [15/15], Loss: 0.6940750479698181\n",
      "Accuracy: 0.4999449823943662\n",
      "Model 11 finished\n",
      "Epoch [1/15], Loss: 0.6509786248207092\n",
      "Epoch [2/15], Loss: 0.657136082649231\n",
      "Epoch [3/15], Loss: 0.6260748505592346\n",
      "Epoch [4/15], Loss: 0.6542625427246094\n",
      "Epoch [5/15], Loss: 0.6077584028244019\n",
      "Epoch [6/15], Loss: 0.6535166501998901\n",
      "Epoch [7/15], Loss: 0.63642418384552\n",
      "Epoch [8/15], Loss: 0.6431059241294861\n",
      "Epoch [9/15], Loss: 0.6299372911453247\n",
      "Epoch [10/15], Loss: 0.6257126927375793\n",
      "Epoch [11/15], Loss: 0.6402674317359924\n",
      "Epoch [12/15], Loss: 0.6461396217346191\n",
      "Epoch [13/15], Loss: 0.6515313386917114\n",
      "Epoch [14/15], Loss: 0.6714464426040649\n",
      "Epoch [15/15], Loss: 0.6172953248023987\n",
      "Accuracy: 0.6827134683098591\n",
      "Model 12 finished\n",
      "Epoch [1/15], Loss: 0.8144650459289551\n",
      "Epoch [2/15], Loss: 0.7560896277427673\n",
      "Epoch [3/15], Loss: 0.8021463751792908\n",
      "Epoch [4/15], Loss: 0.7656922936439514\n",
      "Epoch [5/15], Loss: 0.7739916443824768\n",
      "Epoch [6/15], Loss: 0.7771738767623901\n",
      "Epoch [7/15], Loss: 0.7729642987251282\n",
      "Epoch [8/15], Loss: 0.813646137714386\n",
      "Epoch [9/15], Loss: 0.8031357526779175\n",
      "Epoch [10/15], Loss: 0.76811683177948\n",
      "Epoch [11/15], Loss: 0.7976664304733276\n",
      "Epoch [12/15], Loss: 0.7792841792106628\n",
      "Epoch [13/15], Loss: 0.8094024658203125\n",
      "Epoch [14/15], Loss: 0.7472256422042847\n",
      "Epoch [15/15], Loss: 0.8173004388809204\n",
      "Accuracy: 0.24130721830985916\n",
      "Model 13 finished\n",
      "Epoch [1/15], Loss: 0.6632009744644165\n",
      "Epoch [2/15], Loss: 0.6575257182121277\n",
      "Epoch [3/15], Loss: 0.6510306596755981\n",
      "Epoch [4/15], Loss: 0.6436302661895752\n",
      "Epoch [5/15], Loss: 0.6395041346549988\n",
      "Epoch [6/15], Loss: 0.6568163633346558\n",
      "Epoch [7/15], Loss: 0.6250609159469604\n",
      "Epoch [8/15], Loss: 0.6358895301818848\n",
      "Epoch [9/15], Loss: 0.6591914296150208\n",
      "Epoch [10/15], Loss: 0.6140417456626892\n",
      "Epoch [11/15], Loss: 0.6296507120132446\n",
      "Epoch [12/15], Loss: 0.6741127967834473\n",
      "Epoch [13/15], Loss: 0.6322131156921387\n",
      "Epoch [14/15], Loss: 0.6459839940071106\n",
      "Epoch [15/15], Loss: 0.6534823179244995\n",
      "Accuracy: 0.7581976232394366\n",
      "Model 14 finished\n",
      "Epoch [1/15], Loss: 0.7216634154319763\n",
      "Epoch [2/15], Loss: 0.7295900583267212\n",
      "Epoch [3/15], Loss: 0.7153618931770325\n",
      "Epoch [4/15], Loss: 0.7499451041221619\n",
      "Epoch [5/15], Loss: 0.7010285258293152\n",
      "Epoch [6/15], Loss: 0.7170630097389221\n",
      "Epoch [7/15], Loss: 0.7358500361442566\n",
      "Epoch [8/15], Loss: 0.7297435402870178\n",
      "Epoch [9/15], Loss: 0.7339915633201599\n",
      "Epoch [10/15], Loss: 0.7081723809242249\n",
      "Epoch [11/15], Loss: 0.7114682197570801\n",
      "Epoch [12/15], Loss: 0.7251454591751099\n",
      "Epoch [13/15], Loss: 0.7075339555740356\n",
      "Epoch [14/15], Loss: 0.7141720652580261\n",
      "Epoch [15/15], Loss: 0.7210447192192078\n",
      "Accuracy: 0.30358714788732394\n",
      "Model 15 finished\n",
      "Epoch [1/15], Loss: 0.7464613914489746\n",
      "Epoch [2/15], Loss: 0.8164995908737183\n",
      "Epoch [3/15], Loss: 0.7943600416183472\n",
      "Epoch [4/15], Loss: 0.7912143468856812\n",
      "Epoch [5/15], Loss: 0.7117381691932678\n",
      "Epoch [6/15], Loss: 0.756058394908905\n",
      "Epoch [7/15], Loss: 0.8317440748214722\n",
      "Epoch [8/15], Loss: 0.7694652080535889\n",
      "Epoch [9/15], Loss: 0.7426203489303589\n",
      "Epoch [10/15], Loss: 0.7805576324462891\n",
      "Epoch [11/15], Loss: 0.7419685125350952\n",
      "Epoch [12/15], Loss: 0.7751229405403137\n",
      "Epoch [13/15], Loss: 0.8050864338874817\n",
      "Epoch [14/15], Loss: 0.7318103909492493\n",
      "Epoch [15/15], Loss: 0.8135748505592346\n",
      "Accuracy: 0.2722271126760563\n",
      "Model 16 finished\n",
      "Epoch [1/15], Loss: 0.5549805760383606\n",
      "Epoch [2/15], Loss: 0.6411812901496887\n",
      "Epoch [3/15], Loss: 0.5831831097602844\n",
      "Epoch [4/15], Loss: 0.634128212928772\n",
      "Epoch [5/15], Loss: 0.5631678700447083\n",
      "Epoch [6/15], Loss: 0.5947916507720947\n",
      "Epoch [7/15], Loss: 0.5782884955406189\n",
      "Epoch [8/15], Loss: 0.6080270409584045\n",
      "Epoch [9/15], Loss: 0.6233876347541809\n",
      "Epoch [10/15], Loss: 0.5832117199897766\n",
      "Epoch [11/15], Loss: 0.5736583471298218\n",
      "Epoch [12/15], Loss: 0.5920045971870422\n",
      "Epoch [13/15], Loss: 0.6757588386535645\n",
      "Epoch [14/15], Loss: 0.5485803484916687\n",
      "Epoch [15/15], Loss: 0.5668103098869324\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 17 finished\n",
      "Epoch [1/15], Loss: 0.8166111707687378\n",
      "Epoch [2/15], Loss: 0.8410255908966064\n",
      "Epoch [3/15], Loss: 0.9007052183151245\n",
      "Epoch [4/15], Loss: 0.8502821922302246\n",
      "Epoch [5/15], Loss: 0.8008689880371094\n",
      "Epoch [6/15], Loss: 0.8355937004089355\n",
      "Epoch [7/15], Loss: 0.8116002678871155\n",
      "Epoch [8/15], Loss: 0.8262385129928589\n",
      "Epoch [9/15], Loss: 0.8458241820335388\n",
      "Epoch [10/15], Loss: 0.8789175152778625\n",
      "Epoch [11/15], Loss: 0.8545664548873901\n",
      "Epoch [12/15], Loss: 0.8006410002708435\n",
      "Epoch [13/15], Loss: 0.8370980620384216\n",
      "Epoch [14/15], Loss: 0.7866860628128052\n",
      "Epoch [15/15], Loss: 0.8524575233459473\n",
      "Accuracy: 0.2169344190140845\n",
      "Model 18 finished\n",
      "Epoch [1/15], Loss: 0.7521979808807373\n",
      "Epoch [2/15], Loss: 0.7372293472290039\n",
      "Epoch [3/15], Loss: 0.7606019377708435\n",
      "Epoch [4/15], Loss: 0.7126392722129822\n",
      "Epoch [5/15], Loss: 0.7305105328559875\n",
      "Epoch [6/15], Loss: 0.7228208780288696\n",
      "Epoch [7/15], Loss: 0.7228002548217773\n",
      "Epoch [8/15], Loss: 0.7182703614234924\n",
      "Epoch [9/15], Loss: 0.7586832642555237\n",
      "Epoch [10/15], Loss: 0.7411860227584839\n",
      "Epoch [11/15], Loss: 0.736072301864624\n",
      "Epoch [12/15], Loss: 0.7265508770942688\n",
      "Epoch [13/15], Loss: 0.7335349917411804\n",
      "Epoch [14/15], Loss: 0.7122709155082703\n",
      "Epoch [15/15], Loss: 0.7557960748672485\n",
      "Accuracy: 0.22172095070422534\n",
      "Model 19 finished\n",
      "Epoch [1/15], Loss: 0.7869246602058411\n",
      "Epoch [2/15], Loss: 0.8377709984779358\n",
      "Epoch [3/15], Loss: 0.8530579805374146\n",
      "Epoch [4/15], Loss: 0.8249348998069763\n",
      "Epoch [5/15], Loss: 0.8713472485542297\n",
      "Epoch [6/15], Loss: 0.8059612512588501\n",
      "Epoch [7/15], Loss: 0.7343356013298035\n",
      "Epoch [8/15], Loss: 0.8215025663375854\n",
      "Epoch [9/15], Loss: 0.8707495331764221\n",
      "Epoch [10/15], Loss: 0.8221073150634766\n",
      "Epoch [11/15], Loss: 0.853152871131897\n",
      "Epoch [12/15], Loss: 0.8250754475593567\n",
      "Epoch [13/15], Loss: 0.8284996747970581\n",
      "Epoch [14/15], Loss: 0.7744882702827454\n",
      "Epoch [15/15], Loss: 0.8738967180252075\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 20 finished\n",
      "Epoch [1/15], Loss: 0.7074512839317322\n",
      "Epoch [2/15], Loss: 0.6961461305618286\n",
      "Epoch [3/15], Loss: 0.7238855957984924\n",
      "Epoch [4/15], Loss: 0.719613254070282\n",
      "Epoch [5/15], Loss: 0.71138995885849\n",
      "Epoch [6/15], Loss: 0.7171003222465515\n",
      "Epoch [7/15], Loss: 0.7065426111221313\n",
      "Epoch [8/15], Loss: 0.7114694714546204\n",
      "Epoch [9/15], Loss: 0.6944611668586731\n",
      "Epoch [10/15], Loss: 0.7164513468742371\n",
      "Epoch [11/15], Loss: 0.7227445244789124\n",
      "Epoch [12/15], Loss: 0.7231616973876953\n",
      "Epoch [13/15], Loss: 0.7104353308677673\n",
      "Epoch [14/15], Loss: 0.7231074571609497\n",
      "Epoch [15/15], Loss: 0.7306502461433411\n",
      "Accuracy: 0.2819102112676056\n",
      "Model 21 finished\n",
      "Epoch [1/15], Loss: 0.8004983067512512\n",
      "Epoch [2/15], Loss: 0.7868629693984985\n",
      "Epoch [3/15], Loss: 0.8071451783180237\n",
      "Epoch [4/15], Loss: 0.7449027299880981\n",
      "Epoch [5/15], Loss: 0.7976526618003845\n",
      "Epoch [6/15], Loss: 0.7953919172286987\n",
      "Epoch [7/15], Loss: 0.7756602764129639\n",
      "Epoch [8/15], Loss: 0.7898625731468201\n",
      "Epoch [9/15], Loss: 0.8045504689216614\n",
      "Epoch [10/15], Loss: 0.7818823456764221\n",
      "Epoch [11/15], Loss: 0.742883563041687\n",
      "Epoch [12/15], Loss: 0.7530420422554016\n",
      "Epoch [13/15], Loss: 0.8239985108375549\n",
      "Epoch [14/15], Loss: 0.8193379640579224\n",
      "Epoch [15/15], Loss: 0.8181844353675842\n",
      "Accuracy: 0.22166593309859156\n",
      "Model 22 finished\n",
      "Epoch [1/15], Loss: 0.7414764761924744\n",
      "Epoch [2/15], Loss: 0.8365389704704285\n",
      "Epoch [3/15], Loss: 0.7858465909957886\n",
      "Epoch [4/15], Loss: 0.7673991322517395\n",
      "Epoch [5/15], Loss: 0.8558661341667175\n",
      "Epoch [6/15], Loss: 0.8035287261009216\n",
      "Epoch [7/15], Loss: 0.7985234260559082\n",
      "Epoch [8/15], Loss: 0.8091557621955872\n",
      "Epoch [9/15], Loss: 0.8324096202850342\n",
      "Epoch [10/15], Loss: 0.8443038463592529\n",
      "Epoch [11/15], Loss: 0.8490094542503357\n",
      "Epoch [12/15], Loss: 0.8095518946647644\n",
      "Epoch [13/15], Loss: 0.7904280424118042\n",
      "Epoch [14/15], Loss: 0.8463212847709656\n",
      "Epoch [15/15], Loss: 0.8368946313858032\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 23 finished\n",
      "Epoch [1/15], Loss: 0.6621230840682983\n",
      "Epoch [2/15], Loss: 0.5782623291015625\n",
      "Epoch [3/15], Loss: 0.5281426310539246\n",
      "Epoch [4/15], Loss: 0.5541282296180725\n",
      "Epoch [5/15], Loss: 0.5590425133705139\n",
      "Epoch [6/15], Loss: 0.5642933249473572\n",
      "Epoch [7/15], Loss: 0.6054280996322632\n",
      "Epoch [8/15], Loss: 0.6334237456321716\n",
      "Epoch [9/15], Loss: 0.6226018667221069\n",
      "Epoch [10/15], Loss: 0.5847041010856628\n",
      "Epoch [11/15], Loss: 0.5451329350471497\n",
      "Epoch [12/15], Loss: 0.572425127029419\n",
      "Epoch [13/15], Loss: 0.5737799406051636\n",
      "Epoch [14/15], Loss: 0.5868518352508545\n",
      "Epoch [15/15], Loss: 0.5990632176399231\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 24 finished\n",
      "Epoch [1/15], Loss: 0.7094407081604004\n",
      "Epoch [2/15], Loss: 0.7313488721847534\n",
      "Epoch [3/15], Loss: 0.7422993183135986\n",
      "Epoch [4/15], Loss: 0.7155331373214722\n",
      "Epoch [5/15], Loss: 0.7207819819450378\n",
      "Epoch [6/15], Loss: 0.7100615501403809\n",
      "Epoch [7/15], Loss: 0.7417243719100952\n",
      "Epoch [8/15], Loss: 0.7234793901443481\n",
      "Epoch [9/15], Loss: 0.726579487323761\n",
      "Epoch [10/15], Loss: 0.7275563478469849\n",
      "Epoch [11/15], Loss: 0.7104045152664185\n",
      "Epoch [12/15], Loss: 0.7143353223800659\n",
      "Epoch [13/15], Loss: 0.7116577625274658\n",
      "Epoch [14/15], Loss: 0.7145510315895081\n",
      "Epoch [15/15], Loss: 0.7132270336151123\n",
      "Accuracy: 0.24631382042253522\n",
      "Model 25 finished\n",
      "Epoch [1/15], Loss: 0.6309327483177185\n",
      "Epoch [2/15], Loss: 0.6569714546203613\n",
      "Epoch [3/15], Loss: 0.6529034972190857\n",
      "Epoch [4/15], Loss: 0.6212729811668396\n",
      "Epoch [5/15], Loss: 0.6498185396194458\n",
      "Epoch [6/15], Loss: 0.6158897280693054\n",
      "Epoch [7/15], Loss: 0.62763512134552\n",
      "Epoch [8/15], Loss: 0.6118204593658447\n",
      "Epoch [9/15], Loss: 0.6500585079193115\n",
      "Epoch [10/15], Loss: 0.6472143530845642\n",
      "Epoch [11/15], Loss: 0.652256429195404\n",
      "Epoch [12/15], Loss: 0.6446559429168701\n",
      "Epoch [13/15], Loss: 0.6344416737556458\n",
      "Epoch [14/15], Loss: 0.6422836184501648\n",
      "Epoch [15/15], Loss: 0.6354718804359436\n",
      "Accuracy: 0.7780589788732394\n",
      "Model 26 finished\n",
      "Epoch [1/15], Loss: 0.6749931573867798\n",
      "Epoch [2/15], Loss: 0.6712220311164856\n",
      "Epoch [3/15], Loss: 0.6693963408470154\n",
      "Epoch [4/15], Loss: 0.6754557490348816\n",
      "Epoch [5/15], Loss: 0.6555524468421936\n",
      "Epoch [6/15], Loss: 0.6725316047668457\n",
      "Epoch [7/15], Loss: 0.6544443964958191\n",
      "Epoch [8/15], Loss: 0.6501532793045044\n",
      "Epoch [9/15], Loss: 0.6551288962364197\n",
      "Epoch [10/15], Loss: 0.6836076378822327\n",
      "Epoch [11/15], Loss: 0.6870548129081726\n",
      "Epoch [12/15], Loss: 0.6494324207305908\n",
      "Epoch [13/15], Loss: 0.6579349040985107\n",
      "Epoch [14/15], Loss: 0.6806416511535645\n",
      "Epoch [15/15], Loss: 0.6738516092300415\n",
      "Accuracy: 0.761443661971831\n",
      "Model 27 finished\n",
      "Epoch [1/15], Loss: 0.7206786274909973\n",
      "Epoch [2/15], Loss: 0.7247395515441895\n",
      "Epoch [3/15], Loss: 0.7214218974113464\n",
      "Epoch [4/15], Loss: 0.7332095503807068\n",
      "Epoch [5/15], Loss: 0.7369377017021179\n",
      "Epoch [6/15], Loss: 0.7275295853614807\n",
      "Epoch [7/15], Loss: 0.7371837496757507\n",
      "Epoch [8/15], Loss: 0.7264896035194397\n",
      "Epoch [9/15], Loss: 0.721903920173645\n",
      "Epoch [10/15], Loss: 0.7290641069412231\n",
      "Epoch [11/15], Loss: 0.7230297923088074\n",
      "Epoch [12/15], Loss: 0.7265815734863281\n",
      "Epoch [13/15], Loss: 0.7378321290016174\n",
      "Epoch [14/15], Loss: 0.7432599663734436\n",
      "Epoch [15/15], Loss: 0.7129090428352356\n",
      "Accuracy: 0.22210607394366197\n",
      "Model 28 finished\n",
      "Epoch [1/15], Loss: 0.7706689238548279\n",
      "Epoch [2/15], Loss: 0.7309175133705139\n",
      "Epoch [3/15], Loss: 0.7636783123016357\n",
      "Epoch [4/15], Loss: 0.727812647819519\n",
      "Epoch [5/15], Loss: 0.75762939453125\n",
      "Epoch [6/15], Loss: 0.7472022175788879\n",
      "Epoch [7/15], Loss: 0.7398878931999207\n",
      "Epoch [8/15], Loss: 0.7296499609947205\n",
      "Epoch [9/15], Loss: 0.7265569567680359\n",
      "Epoch [10/15], Loss: 0.777983546257019\n",
      "Epoch [11/15], Loss: 0.7399793863296509\n",
      "Epoch [12/15], Loss: 0.7170186042785645\n",
      "Epoch [13/15], Loss: 0.7242364883422852\n",
      "Epoch [14/15], Loss: 0.7575844526290894\n",
      "Epoch [15/15], Loss: 0.7269333004951477\n",
      "Accuracy: 0.2340448943661972\n",
      "Model 29 finished\n",
      "Epoch [1/15], Loss: 0.6364090442657471\n",
      "Epoch [2/15], Loss: 0.6843693256378174\n",
      "Epoch [3/15], Loss: 0.627166748046875\n",
      "Epoch [4/15], Loss: 0.6338408589363098\n",
      "Epoch [5/15], Loss: 0.6366857886314392\n",
      "Epoch [6/15], Loss: 0.6161899566650391\n",
      "Epoch [7/15], Loss: 0.6585832238197327\n",
      "Epoch [8/15], Loss: 0.6453400254249573\n",
      "Epoch [9/15], Loss: 0.6332405805587769\n",
      "Epoch [10/15], Loss: 0.6464207768440247\n",
      "Epoch [11/15], Loss: 0.6676756143569946\n",
      "Epoch [12/15], Loss: 0.6166911125183105\n",
      "Epoch [13/15], Loss: 0.6421464085578918\n",
      "Epoch [14/15], Loss: 0.6441119909286499\n",
      "Epoch [15/15], Loss: 0.6314668655395508\n",
      "Accuracy: 0.7781139964788732\n",
      "Model 30 finished\n",
      "Epoch [1/15], Loss: 0.7683268189430237\n",
      "Epoch [2/15], Loss: 0.7516260147094727\n",
      "Epoch [3/15], Loss: 0.7380560636520386\n",
      "Epoch [4/15], Loss: 0.7612339854240417\n",
      "Epoch [5/15], Loss: 0.7484334707260132\n",
      "Epoch [6/15], Loss: 0.7664975523948669\n",
      "Epoch [7/15], Loss: 0.7669324278831482\n",
      "Epoch [8/15], Loss: 0.7615165710449219\n",
      "Epoch [9/15], Loss: 0.7524611353874207\n",
      "Epoch [10/15], Loss: 0.7714722156524658\n",
      "Epoch [11/15], Loss: 0.7461056709289551\n",
      "Epoch [12/15], Loss: 0.761138379573822\n",
      "Epoch [13/15], Loss: 0.7239862084388733\n",
      "Epoch [14/15], Loss: 0.7521854639053345\n",
      "Epoch [15/15], Loss: 0.770696759223938\n",
      "Accuracy: 0.22183098591549297\n",
      "Model 31 finished\n",
      "Epoch [1/15], Loss: 0.6947950720787048\n",
      "Epoch [2/15], Loss: 0.7049334645271301\n",
      "Epoch [3/15], Loss: 0.6826971769332886\n",
      "Epoch [4/15], Loss: 0.6635876297950745\n",
      "Epoch [5/15], Loss: 0.6876775026321411\n",
      "Epoch [6/15], Loss: 0.6880442500114441\n",
      "Epoch [7/15], Loss: 0.6840608716011047\n",
      "Epoch [8/15], Loss: 0.68708735704422\n",
      "Epoch [9/15], Loss: 0.6784461140632629\n",
      "Epoch [10/15], Loss: 0.6699784994125366\n",
      "Epoch [11/15], Loss: 0.6678156852722168\n",
      "Epoch [12/15], Loss: 0.6828988790512085\n",
      "Epoch [13/15], Loss: 0.6910382509231567\n",
      "Epoch [14/15], Loss: 0.6765273809432983\n",
      "Epoch [15/15], Loss: 0.6848458647727966\n",
      "Accuracy: 0.5234925176056338\n",
      "Model 32 finished\n",
      "Epoch [1/15], Loss: 0.7398369908332825\n",
      "Epoch [2/15], Loss: 0.7405393719673157\n",
      "Epoch [3/15], Loss: 0.7249998450279236\n",
      "Epoch [4/15], Loss: 0.7243472337722778\n",
      "Epoch [5/15], Loss: 0.7199338674545288\n",
      "Epoch [6/15], Loss: 0.7351617217063904\n",
      "Epoch [7/15], Loss: 0.7346154451370239\n",
      "Epoch [8/15], Loss: 0.7178325057029724\n",
      "Epoch [9/15], Loss: 0.7183084487915039\n",
      "Epoch [10/15], Loss: 0.7268737554550171\n",
      "Epoch [11/15], Loss: 0.7203460931777954\n",
      "Epoch [12/15], Loss: 0.7340344190597534\n",
      "Epoch [13/15], Loss: 0.7274785041809082\n",
      "Epoch [14/15], Loss: 0.7219616174697876\n",
      "Epoch [15/15], Loss: 0.7325359582901001\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 33 finished\n",
      "Epoch [1/15], Loss: 0.6577877998352051\n",
      "Epoch [2/15], Loss: 0.6657732129096985\n",
      "Epoch [3/15], Loss: 0.6550705432891846\n",
      "Epoch [4/15], Loss: 0.6661251187324524\n",
      "Epoch [5/15], Loss: 0.6780773997306824\n",
      "Epoch [6/15], Loss: 0.6608818769454956\n",
      "Epoch [7/15], Loss: 0.6646729707717896\n",
      "Epoch [8/15], Loss: 0.6894116401672363\n",
      "Epoch [9/15], Loss: 0.680008590221405\n",
      "Epoch [10/15], Loss: 0.6705117225646973\n",
      "Epoch [11/15], Loss: 0.6694031953811646\n",
      "Epoch [12/15], Loss: 0.6718521118164062\n",
      "Epoch [13/15], Loss: 0.6786158084869385\n",
      "Epoch [14/15], Loss: 0.6736437678337097\n",
      "Epoch [15/15], Loss: 0.6690037846565247\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 34 finished\n",
      "Epoch [1/15], Loss: 0.7661883234977722\n",
      "Epoch [2/15], Loss: 0.7294972538948059\n",
      "Epoch [3/15], Loss: 0.8139626383781433\n",
      "Epoch [4/15], Loss: 0.8074755072593689\n",
      "Epoch [5/15], Loss: 0.7921267151832581\n",
      "Epoch [6/15], Loss: 0.7780957818031311\n",
      "Epoch [7/15], Loss: 0.8179998993873596\n",
      "Epoch [8/15], Loss: 0.7702538371086121\n",
      "Epoch [9/15], Loss: 0.7899632453918457\n",
      "Epoch [10/15], Loss: 0.8165575861930847\n",
      "Epoch [11/15], Loss: 0.8257096409797668\n",
      "Epoch [12/15], Loss: 0.7666695713996887\n",
      "Epoch [13/15], Loss: 0.7848109006881714\n",
      "Epoch [14/15], Loss: 0.7630394697189331\n",
      "Epoch [15/15], Loss: 0.7637694478034973\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 35 finished\n",
      "Epoch [1/15], Loss: 0.7628086805343628\n",
      "Epoch [2/15], Loss: 0.7677965760231018\n",
      "Epoch [3/15], Loss: 0.8047998547554016\n",
      "Epoch [4/15], Loss: 0.7596452236175537\n",
      "Epoch [5/15], Loss: 0.7443653345108032\n",
      "Epoch [6/15], Loss: 0.7814237475395203\n",
      "Epoch [7/15], Loss: 0.7424389123916626\n",
      "Epoch [8/15], Loss: 0.7601931691169739\n",
      "Epoch [9/15], Loss: 0.7522810697555542\n",
      "Epoch [10/15], Loss: 0.7823056578636169\n",
      "Epoch [11/15], Loss: 0.7471101880073547\n",
      "Epoch [12/15], Loss: 0.7608628869056702\n",
      "Epoch [13/15], Loss: 0.7848113179206848\n",
      "Epoch [14/15], Loss: 0.7355717420578003\n",
      "Epoch [15/15], Loss: 0.7622086405754089\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 36 finished\n",
      "Epoch [1/15], Loss: 1.0042481422424316\n",
      "Epoch [2/15], Loss: 0.9987998008728027\n",
      "Epoch [3/15], Loss: 0.9301229119300842\n",
      "Epoch [4/15], Loss: 0.8982729315757751\n",
      "Epoch [5/15], Loss: 0.8657304644584656\n",
      "Epoch [6/15], Loss: 0.9202882051467896\n",
      "Epoch [7/15], Loss: 0.8782137036323547\n",
      "Epoch [8/15], Loss: 0.9694054126739502\n",
      "Epoch [9/15], Loss: 0.9519217610359192\n",
      "Epoch [10/15], Loss: 0.8752123713493347\n",
      "Epoch [11/15], Loss: 0.902803897857666\n",
      "Epoch [12/15], Loss: 0.9147189855575562\n",
      "Epoch [13/15], Loss: 0.9139969944953918\n",
      "Epoch [14/15], Loss: 0.994086503982544\n",
      "Epoch [15/15], Loss: 0.9163683652877808\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 37 finished\n",
      "Epoch [1/15], Loss: 0.9485995173454285\n",
      "Epoch [2/15], Loss: 0.8621065020561218\n",
      "Epoch [3/15], Loss: 1.031363844871521\n",
      "Epoch [4/15], Loss: 0.943431556224823\n",
      "Epoch [5/15], Loss: 0.8986263275146484\n",
      "Epoch [6/15], Loss: 0.842930018901825\n",
      "Epoch [7/15], Loss: 0.9288919568061829\n",
      "Epoch [8/15], Loss: 0.9338609576225281\n",
      "Epoch [9/15], Loss: 0.9562649726867676\n",
      "Epoch [10/15], Loss: 0.9434766173362732\n",
      "Epoch [11/15], Loss: 0.9274439811706543\n",
      "Epoch [12/15], Loss: 0.967290997505188\n",
      "Epoch [13/15], Loss: 1.0020544528961182\n",
      "Epoch [14/15], Loss: 0.9733895659446716\n",
      "Epoch [15/15], Loss: 0.9512930512428284\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 38 finished\n",
      "Epoch [1/15], Loss: 0.6988378763198853\n",
      "Epoch [2/15], Loss: 0.7011562585830688\n",
      "Epoch [3/15], Loss: 0.6994006633758545\n",
      "Epoch [4/15], Loss: 0.6981227397918701\n",
      "Epoch [5/15], Loss: 0.7155261039733887\n",
      "Epoch [6/15], Loss: 0.7021142840385437\n",
      "Epoch [7/15], Loss: 0.6968203186988831\n",
      "Epoch [8/15], Loss: 0.7051776051521301\n",
      "Epoch [9/15], Loss: 0.7030388712882996\n",
      "Epoch [10/15], Loss: 0.6917769312858582\n",
      "Epoch [11/15], Loss: 0.707237958908081\n",
      "Epoch [12/15], Loss: 0.7086377739906311\n",
      "Epoch [13/15], Loss: 0.6970745921134949\n",
      "Epoch [14/15], Loss: 0.7085989117622375\n",
      "Epoch [15/15], Loss: 0.7024145126342773\n",
      "Accuracy: 0.22942341549295775\n",
      "Model 39 finished\n",
      "Epoch [1/15], Loss: 0.6928543448448181\n",
      "Epoch [2/15], Loss: 0.6896597743034363\n",
      "Epoch [3/15], Loss: 0.720285952091217\n",
      "Epoch [4/15], Loss: 0.7177729606628418\n",
      "Epoch [5/15], Loss: 0.6998196244239807\n",
      "Epoch [6/15], Loss: 0.7216616272926331\n",
      "Epoch [7/15], Loss: 0.6869425177574158\n",
      "Epoch [8/15], Loss: 0.7132651209831238\n",
      "Epoch [9/15], Loss: 0.6917781829833984\n",
      "Epoch [10/15], Loss: 0.6898584365844727\n",
      "Epoch [11/15], Loss: 0.7007262110710144\n",
      "Epoch [12/15], Loss: 0.688065767288208\n",
      "Epoch [13/15], Loss: 0.6798309087753296\n",
      "Epoch [14/15], Loss: 0.6897196769714355\n",
      "Epoch [15/15], Loss: 0.6842411756515503\n",
      "Accuracy: 0.6939370598591549\n",
      "Model 40 finished\n",
      "Epoch [1/15], Loss: 0.6500281691551208\n",
      "Epoch [2/15], Loss: 0.6733394861221313\n",
      "Epoch [3/15], Loss: 0.6562944054603577\n",
      "Epoch [4/15], Loss: 0.64909827709198\n",
      "Epoch [5/15], Loss: 0.6542468070983887\n",
      "Epoch [6/15], Loss: 0.6630312204360962\n",
      "Epoch [7/15], Loss: 0.6731598377227783\n",
      "Epoch [8/15], Loss: 0.6396576166152954\n",
      "Epoch [9/15], Loss: 0.6594669222831726\n",
      "Epoch [10/15], Loss: 0.663934588432312\n",
      "Epoch [11/15], Loss: 0.6566018462181091\n",
      "Epoch [12/15], Loss: 0.6343628764152527\n",
      "Epoch [13/15], Loss: 0.6473473310470581\n",
      "Epoch [14/15], Loss: 0.6284101605415344\n",
      "Epoch [15/15], Loss: 0.6573103070259094\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 41 finished\n",
      "Epoch [1/15], Loss: 0.644193708896637\n",
      "Epoch [2/15], Loss: 0.6408036351203918\n",
      "Epoch [3/15], Loss: 0.6685852408409119\n",
      "Epoch [4/15], Loss: 0.6727402806282043\n",
      "Epoch [5/15], Loss: 0.6245328187942505\n",
      "Epoch [6/15], Loss: 0.6580769419670105\n",
      "Epoch [7/15], Loss: 0.6640641093254089\n",
      "Epoch [8/15], Loss: 0.6223464012145996\n",
      "Epoch [9/15], Loss: 0.6468415260314941\n",
      "Epoch [10/15], Loss: 0.6424442529678345\n",
      "Epoch [11/15], Loss: 0.6341172456741333\n",
      "Epoch [12/15], Loss: 0.6540924310684204\n",
      "Epoch [13/15], Loss: 0.6436001062393188\n",
      "Epoch [14/15], Loss: 0.6606741547584534\n",
      "Epoch [15/15], Loss: 0.6443520784378052\n",
      "Accuracy: 0.778169014084507\n",
      "Model 42 finished\n",
      "Epoch [1/15], Loss: 0.6668795943260193\n",
      "Epoch [2/15], Loss: 0.6657134294509888\n",
      "Epoch [3/15], Loss: 0.6650960445404053\n",
      "Epoch [4/15], Loss: 0.6826009154319763\n",
      "Epoch [5/15], Loss: 0.665922999382019\n",
      "Epoch [6/15], Loss: 0.6637207269668579\n",
      "Epoch [7/15], Loss: 0.6552091836929321\n",
      "Epoch [8/15], Loss: 0.6686469912528992\n",
      "Epoch [9/15], Loss: 0.6670933961868286\n",
      "Epoch [10/15], Loss: 0.67315673828125\n",
      "Epoch [11/15], Loss: 0.6656298637390137\n",
      "Epoch [12/15], Loss: 0.675586998462677\n",
      "Epoch [13/15], Loss: 0.6580272316932678\n",
      "Epoch [14/15], Loss: 0.6560935974121094\n",
      "Epoch [15/15], Loss: 0.6428841948509216\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 43 finished\n",
      "Epoch [1/15], Loss: 0.6156286597251892\n",
      "Epoch [2/15], Loss: 0.6178668141365051\n",
      "Epoch [3/15], Loss: 0.6322498321533203\n",
      "Epoch [4/15], Loss: 0.5892441868782043\n",
      "Epoch [5/15], Loss: 0.598168671131134\n",
      "Epoch [6/15], Loss: 0.5821596384048462\n",
      "Epoch [7/15], Loss: 0.6460000276565552\n",
      "Epoch [8/15], Loss: 0.5948924422264099\n",
      "Epoch [9/15], Loss: 0.5981365442276001\n",
      "Epoch [10/15], Loss: 0.6259068846702576\n",
      "Epoch [11/15], Loss: 0.5948892831802368\n",
      "Epoch [12/15], Loss: 0.6258136630058289\n",
      "Epoch [13/15], Loss: 0.6252099275588989\n",
      "Epoch [14/15], Loss: 0.616921067237854\n",
      "Epoch [15/15], Loss: 0.5653340220451355\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 44 finished\n",
      "Epoch [1/15], Loss: 0.7850621342658997\n",
      "Epoch [2/15], Loss: 0.7887490391731262\n",
      "Epoch [3/15], Loss: 0.8342738151550293\n",
      "Epoch [4/15], Loss: 0.7975956201553345\n",
      "Epoch [5/15], Loss: 0.8065968751907349\n",
      "Epoch [6/15], Loss: 0.7739655375480652\n",
      "Epoch [7/15], Loss: 0.814149022102356\n",
      "Epoch [8/15], Loss: 0.8034940361976624\n",
      "Epoch [9/15], Loss: 0.8541603088378906\n",
      "Epoch [10/15], Loss: 0.7994745969772339\n",
      "Epoch [11/15], Loss: 0.7465426325798035\n",
      "Epoch [12/15], Loss: 0.825775682926178\n",
      "Epoch [13/15], Loss: 0.8171420097351074\n",
      "Epoch [14/15], Loss: 0.7769643664360046\n",
      "Epoch [15/15], Loss: 0.813493549823761\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 45 finished\n",
      "Epoch [1/15], Loss: 0.6758556365966797\n",
      "Epoch [2/15], Loss: 0.684942901134491\n",
      "Epoch [3/15], Loss: 0.6724742650985718\n",
      "Epoch [4/15], Loss: 0.6725045442581177\n",
      "Epoch [5/15], Loss: 0.6746285557746887\n",
      "Epoch [6/15], Loss: 0.6834431886672974\n",
      "Epoch [7/15], Loss: 0.685522735118866\n",
      "Epoch [8/15], Loss: 0.6830178499221802\n",
      "Epoch [9/15], Loss: 0.6843280792236328\n",
      "Epoch [10/15], Loss: 0.6745113730430603\n",
      "Epoch [11/15], Loss: 0.6779488921165466\n",
      "Epoch [12/15], Loss: 0.6697460412979126\n",
      "Epoch [13/15], Loss: 0.6752533912658691\n",
      "Epoch [14/15], Loss: 0.6707793474197388\n",
      "Epoch [15/15], Loss: 0.673453688621521\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 46 finished\n",
      "Epoch [1/15], Loss: 0.7647925019264221\n",
      "Epoch [2/15], Loss: 0.7630575299263\n",
      "Epoch [3/15], Loss: 0.7776654362678528\n",
      "Epoch [4/15], Loss: 0.7360814809799194\n",
      "Epoch [5/15], Loss: 0.746662974357605\n",
      "Epoch [6/15], Loss: 0.7232113480567932\n",
      "Epoch [7/15], Loss: 0.7513909339904785\n",
      "Epoch [8/15], Loss: 0.7622668147087097\n",
      "Epoch [9/15], Loss: 0.7608720064163208\n",
      "Epoch [10/15], Loss: 0.7103803753852844\n",
      "Epoch [11/15], Loss: 0.7511741518974304\n",
      "Epoch [12/15], Loss: 0.7385096549987793\n",
      "Epoch [13/15], Loss: 0.7910588383674622\n",
      "Epoch [14/15], Loss: 0.7627065777778625\n",
      "Epoch [15/15], Loss: 0.7518292665481567\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 47 finished\n",
      "Epoch [1/15], Loss: 0.7101790308952332\n",
      "Epoch [2/15], Loss: 0.7020667791366577\n",
      "Epoch [3/15], Loss: 0.7014690637588501\n",
      "Epoch [4/15], Loss: 0.7178144454956055\n",
      "Epoch [5/15], Loss: 0.7387464642524719\n",
      "Epoch [6/15], Loss: 0.6998319625854492\n",
      "Epoch [7/15], Loss: 0.7188838124275208\n",
      "Epoch [8/15], Loss: 0.7231867909431458\n",
      "Epoch [9/15], Loss: 0.7048609852790833\n",
      "Epoch [10/15], Loss: 0.7237680554389954\n",
      "Epoch [11/15], Loss: 0.7140792608261108\n",
      "Epoch [12/15], Loss: 0.7158223390579224\n",
      "Epoch [13/15], Loss: 0.7234593629837036\n",
      "Epoch [14/15], Loss: 0.7270640134811401\n",
      "Epoch [15/15], Loss: 0.7311733365058899\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 48 finished\n",
      "Epoch [1/15], Loss: 0.7002802491188049\n",
      "Epoch [2/15], Loss: 0.702456533908844\n",
      "Epoch [3/15], Loss: 0.7009710669517517\n",
      "Epoch [4/15], Loss: 0.7054810523986816\n",
      "Epoch [5/15], Loss: 0.706256628036499\n",
      "Epoch [6/15], Loss: 0.7065061330795288\n",
      "Epoch [7/15], Loss: 0.7226076722145081\n",
      "Epoch [8/15], Loss: 0.7013829946517944\n",
      "Epoch [9/15], Loss: 0.7073948979377747\n",
      "Epoch [10/15], Loss: 0.6806600093841553\n",
      "Epoch [11/15], Loss: 0.7071546316146851\n",
      "Epoch [12/15], Loss: 0.7065844535827637\n",
      "Epoch [13/15], Loss: 0.7061631083488464\n",
      "Epoch [14/15], Loss: 0.7100201845169067\n",
      "Epoch [15/15], Loss: 0.7070659399032593\n",
      "Accuracy: 0.2303587147887324\n",
      "Model 49 finished\n",
      "Epoch [1/15], Loss: 0.6776711940765381\n",
      "Epoch [2/15], Loss: 0.7060014009475708\n",
      "Epoch [3/15], Loss: 0.6723954081535339\n",
      "Epoch [4/15], Loss: 0.6601815223693848\n",
      "Epoch [5/15], Loss: 0.6693388223648071\n",
      "Epoch [6/15], Loss: 0.6685751676559448\n",
      "Epoch [7/15], Loss: 0.6662613749504089\n",
      "Epoch [8/15], Loss: 0.6555915474891663\n",
      "Epoch [9/15], Loss: 0.648129403591156\n",
      "Epoch [10/15], Loss: 0.6746096611022949\n",
      "Epoch [11/15], Loss: 0.6696585416793823\n",
      "Epoch [12/15], Loss: 0.6607245802879333\n",
      "Epoch [13/15], Loss: 0.6639046669006348\n",
      "Epoch [14/15], Loss: 0.6634977459907532\n",
      "Epoch [15/15], Loss: 0.6474111676216125\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 50 finished\n",
      "Epoch [1/15], Loss: 0.5757103562355042\n",
      "Epoch [2/15], Loss: 0.601874828338623\n",
      "Epoch [3/15], Loss: 0.6933715343475342\n",
      "Epoch [4/15], Loss: 0.560195803642273\n",
      "Epoch [5/15], Loss: 0.5728579759597778\n",
      "Epoch [6/15], Loss: 0.5761470198631287\n",
      "Epoch [7/15], Loss: 0.622328519821167\n",
      "Epoch [8/15], Loss: 0.6797636151313782\n",
      "Epoch [9/15], Loss: 0.6532957553863525\n",
      "Epoch [10/15], Loss: 0.5476455092430115\n",
      "Epoch [11/15], Loss: 0.5433667302131653\n",
      "Epoch [12/15], Loss: 0.5804395079612732\n",
      "Epoch [13/15], Loss: 0.6417518854141235\n",
      "Epoch [14/15], Loss: 0.6064969897270203\n",
      "Epoch [15/15], Loss: 0.6286041736602783\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 51 finished\n",
      "Epoch [1/15], Loss: 0.7624638676643372\n",
      "Epoch [2/15], Loss: 0.7417957782745361\n",
      "Epoch [3/15], Loss: 0.7160829305648804\n",
      "Epoch [4/15], Loss: 0.7540758848190308\n",
      "Epoch [5/15], Loss: 0.7391999959945679\n",
      "Epoch [6/15], Loss: 0.7460166215896606\n",
      "Epoch [7/15], Loss: 0.773036777973175\n",
      "Epoch [8/15], Loss: 0.759737491607666\n",
      "Epoch [9/15], Loss: 0.7884207367897034\n",
      "Epoch [10/15], Loss: 0.7486101984977722\n",
      "Epoch [11/15], Loss: 0.7190802693367004\n",
      "Epoch [12/15], Loss: 0.7679257988929749\n",
      "Epoch [13/15], Loss: 0.7564135193824768\n",
      "Epoch [14/15], Loss: 0.7853398323059082\n",
      "Epoch [15/15], Loss: 0.7490167617797852\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 52 finished\n",
      "Epoch [1/15], Loss: 0.8425865173339844\n",
      "Epoch [2/15], Loss: 0.882673442363739\n",
      "Epoch [3/15], Loss: 0.8551446795463562\n",
      "Epoch [4/15], Loss: 0.8772715330123901\n",
      "Epoch [5/15], Loss: 0.9002267122268677\n",
      "Epoch [6/15], Loss: 0.830416202545166\n",
      "Epoch [7/15], Loss: 0.7868983149528503\n",
      "Epoch [8/15], Loss: 0.8289310932159424\n",
      "Epoch [9/15], Loss: 0.9040201306343079\n",
      "Epoch [10/15], Loss: 0.8215894103050232\n",
      "Epoch [11/15], Loss: 0.8480632305145264\n",
      "Epoch [12/15], Loss: 0.8343755006790161\n",
      "Epoch [13/15], Loss: 0.9208335876464844\n",
      "Epoch [14/15], Loss: 0.828576922416687\n",
      "Epoch [15/15], Loss: 0.8302855491638184\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 53 finished\n",
      "Epoch [1/15], Loss: 0.7389378547668457\n",
      "Epoch [2/15], Loss: 0.6940053105354309\n",
      "Epoch [3/15], Loss: 0.7138350605964661\n",
      "Epoch [4/15], Loss: 0.6890043616294861\n",
      "Epoch [5/15], Loss: 0.715251624584198\n",
      "Epoch [6/15], Loss: 0.699724018573761\n",
      "Epoch [7/15], Loss: 0.7403624653816223\n",
      "Epoch [8/15], Loss: 0.7115990519523621\n",
      "Epoch [9/15], Loss: 0.7085604071617126\n",
      "Epoch [10/15], Loss: 0.7283718585968018\n",
      "Epoch [11/15], Loss: 0.7003123164176941\n",
      "Epoch [12/15], Loss: 0.7012596726417542\n",
      "Epoch [13/15], Loss: 0.714461624622345\n",
      "Epoch [14/15], Loss: 0.7031676173210144\n",
      "Epoch [15/15], Loss: 0.7161694169044495\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 54 finished\n",
      "Epoch [1/15], Loss: 0.7639321088790894\n",
      "Epoch [2/15], Loss: 0.8177602887153625\n",
      "Epoch [3/15], Loss: 0.7657762169837952\n",
      "Epoch [4/15], Loss: 0.7573163509368896\n",
      "Epoch [5/15], Loss: 0.8025259375572205\n",
      "Epoch [6/15], Loss: 0.795539379119873\n",
      "Epoch [7/15], Loss: 0.7815293073654175\n",
      "Epoch [8/15], Loss: 0.7783637046813965\n",
      "Epoch [9/15], Loss: 0.7709402441978455\n",
      "Epoch [10/15], Loss: 0.8211552500724792\n",
      "Epoch [11/15], Loss: 0.7815127372741699\n",
      "Epoch [12/15], Loss: 0.8124836683273315\n",
      "Epoch [13/15], Loss: 0.782271146774292\n",
      "Epoch [14/15], Loss: 0.8131040930747986\n",
      "Epoch [15/15], Loss: 0.8185021281242371\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 55 finished\n",
      "Epoch [1/15], Loss: 0.7526271343231201\n",
      "Epoch [2/15], Loss: 0.7500729560852051\n",
      "Epoch [3/15], Loss: 0.7393803596496582\n",
      "Epoch [4/15], Loss: 0.7535331845283508\n",
      "Epoch [5/15], Loss: 0.7378003001213074\n",
      "Epoch [6/15], Loss: 0.7221731543540955\n",
      "Epoch [7/15], Loss: 0.7493342161178589\n",
      "Epoch [8/15], Loss: 0.7592994570732117\n",
      "Epoch [9/15], Loss: 0.7528752684593201\n",
      "Epoch [10/15], Loss: 0.7558501958847046\n",
      "Epoch [11/15], Loss: 0.7431315779685974\n",
      "Epoch [12/15], Loss: 0.7521713972091675\n",
      "Epoch [13/15], Loss: 0.7151038646697998\n",
      "Epoch [14/15], Loss: 0.7440136671066284\n",
      "Epoch [15/15], Loss: 0.7244740724563599\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 56 finished\n",
      "Epoch [1/15], Loss: 0.5581480860710144\n",
      "Epoch [2/15], Loss: 0.6263410449028015\n",
      "Epoch [3/15], Loss: 0.5730719566345215\n",
      "Epoch [4/15], Loss: 0.5968553423881531\n",
      "Epoch [5/15], Loss: 0.558840274810791\n",
      "Epoch [6/15], Loss: 0.5902148485183716\n",
      "Epoch [7/15], Loss: 0.5999806523323059\n",
      "Epoch [8/15], Loss: 0.5570546388626099\n",
      "Epoch [9/15], Loss: 0.5816275477409363\n",
      "Epoch [10/15], Loss: 0.6328669786453247\n",
      "Epoch [11/15], Loss: 0.5703421831130981\n",
      "Epoch [12/15], Loss: 0.5849515795707703\n",
      "Epoch [13/15], Loss: 0.56815505027771\n",
      "Epoch [14/15], Loss: 0.6351196765899658\n",
      "Epoch [15/15], Loss: 0.6167054176330566\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 57 finished\n",
      "Epoch [1/15], Loss: 0.9356673359870911\n",
      "Epoch [2/15], Loss: 0.9402926564216614\n",
      "Epoch [3/15], Loss: 0.8901353478431702\n",
      "Epoch [4/15], Loss: 0.9286893010139465\n",
      "Epoch [5/15], Loss: 0.89857417345047\n",
      "Epoch [6/15], Loss: 0.9535860419273376\n",
      "Epoch [7/15], Loss: 0.9496785998344421\n",
      "Epoch [8/15], Loss: 0.8992716670036316\n",
      "Epoch [9/15], Loss: 0.9040912389755249\n",
      "Epoch [10/15], Loss: 0.8934105634689331\n",
      "Epoch [11/15], Loss: 0.9012846350669861\n",
      "Epoch [12/15], Loss: 0.8130699396133423\n",
      "Epoch [13/15], Loss: 0.8931746482849121\n",
      "Epoch [14/15], Loss: 0.9280131459236145\n",
      "Epoch [15/15], Loss: 0.8660085797309875\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 58 finished\n",
      "Epoch [1/15], Loss: 0.7280398607254028\n",
      "Epoch [2/15], Loss: 0.7021478414535522\n",
      "Epoch [3/15], Loss: 0.726387619972229\n",
      "Epoch [4/15], Loss: 0.7199448943138123\n",
      "Epoch [5/15], Loss: 0.7382819056510925\n",
      "Epoch [6/15], Loss: 0.7200074791908264\n",
      "Epoch [7/15], Loss: 0.7116245627403259\n",
      "Epoch [8/15], Loss: 0.7380441427230835\n",
      "Epoch [9/15], Loss: 0.7089111804962158\n",
      "Epoch [10/15], Loss: 0.7151081562042236\n",
      "Epoch [11/15], Loss: 0.7356542944908142\n",
      "Epoch [12/15], Loss: 0.7192248702049255\n",
      "Epoch [13/15], Loss: 0.7198570966720581\n",
      "Epoch [14/15], Loss: 0.7159075736999512\n",
      "Epoch [15/15], Loss: 0.741457998752594\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 59 finished\n",
      "Epoch [1/15], Loss: 0.8787590265274048\n",
      "Epoch [2/15], Loss: 0.8524573445320129\n",
      "Epoch [3/15], Loss: 0.8656870722770691\n",
      "Epoch [4/15], Loss: 0.8171003460884094\n",
      "Epoch [5/15], Loss: 0.872620165348053\n",
      "Epoch [6/15], Loss: 0.8194563984870911\n",
      "Epoch [7/15], Loss: 0.8886228203773499\n",
      "Epoch [8/15], Loss: 0.8358414173126221\n",
      "Epoch [9/15], Loss: 0.881902813911438\n",
      "Epoch [10/15], Loss: 0.8130411505699158\n",
      "Epoch [11/15], Loss: 0.9565454125404358\n",
      "Epoch [12/15], Loss: 0.8407574892044067\n",
      "Epoch [13/15], Loss: 0.8275538682937622\n",
      "Epoch [14/15], Loss: 0.8647029399871826\n",
      "Epoch [15/15], Loss: 0.8610692024230957\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 60 finished\n",
      "Epoch [1/15], Loss: 0.7279191613197327\n",
      "Epoch [2/15], Loss: 0.7232645153999329\n",
      "Epoch [3/15], Loss: 0.7301076054573059\n",
      "Epoch [4/15], Loss: 0.7214237451553345\n",
      "Epoch [5/15], Loss: 0.7262454032897949\n",
      "Epoch [6/15], Loss: 0.7095209956169128\n",
      "Epoch [7/15], Loss: 0.7020906209945679\n",
      "Epoch [8/15], Loss: 0.7210082411766052\n",
      "Epoch [9/15], Loss: 0.7085924744606018\n",
      "Epoch [10/15], Loss: 0.7111859917640686\n",
      "Epoch [11/15], Loss: 0.7166802287101746\n",
      "Epoch [12/15], Loss: 0.7329951524734497\n",
      "Epoch [13/15], Loss: 0.7098219990730286\n",
      "Epoch [14/15], Loss: 0.7322843670845032\n",
      "Epoch [15/15], Loss: 0.7203561663627625\n",
      "Accuracy: 0.22177596830985916\n",
      "Model 61 finished\n",
      "Epoch [1/15], Loss: 0.673974871635437\n",
      "Epoch [2/15], Loss: 0.6939030289649963\n",
      "Epoch [3/15], Loss: 0.6707448959350586\n",
      "Epoch [4/15], Loss: 0.6908282041549683\n",
      "Epoch [5/15], Loss: 0.6827232241630554\n",
      "Epoch [6/15], Loss: 0.673114001750946\n",
      "Epoch [7/15], Loss: 0.6764412522315979\n",
      "Epoch [8/15], Loss: 0.6919745206832886\n",
      "Epoch [9/15], Loss: 0.6701858043670654\n",
      "Epoch [10/15], Loss: 0.6914313435554504\n",
      "Epoch [11/15], Loss: 0.6846144795417786\n",
      "Epoch [12/15], Loss: 0.6779511570930481\n",
      "Epoch [13/15], Loss: 0.6561232805252075\n",
      "Epoch [14/15], Loss: 0.6862047910690308\n",
      "Epoch [15/15], Loss: 0.6694145798683167\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 62 finished\n",
      "Epoch [1/15], Loss: 0.6484369039535522\n",
      "Epoch [2/15], Loss: 0.6254874467849731\n",
      "Epoch [3/15], Loss: 0.6336342096328735\n",
      "Epoch [4/15], Loss: 0.6267601847648621\n",
      "Epoch [5/15], Loss: 0.6233891248703003\n",
      "Epoch [6/15], Loss: 0.6476941704750061\n",
      "Epoch [7/15], Loss: 0.6265814900398254\n",
      "Epoch [8/15], Loss: 0.6252419352531433\n",
      "Epoch [9/15], Loss: 0.6497013568878174\n",
      "Epoch [10/15], Loss: 0.6430662870407104\n",
      "Epoch [11/15], Loss: 0.6160593032836914\n",
      "Epoch [12/15], Loss: 0.6619771122932434\n",
      "Epoch [13/15], Loss: 0.6257606744766235\n",
      "Epoch [14/15], Loss: 0.6194189190864563\n",
      "Epoch [15/15], Loss: 0.650877833366394\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 63 finished\n",
      "Epoch [1/15], Loss: 0.5341601371765137\n",
      "Epoch [2/15], Loss: 0.5961233377456665\n",
      "Epoch [3/15], Loss: 0.5979287624359131\n",
      "Epoch [4/15], Loss: 0.5719287395477295\n",
      "Epoch [5/15], Loss: 0.6315212845802307\n",
      "Epoch [6/15], Loss: 0.5155320763587952\n",
      "Epoch [7/15], Loss: 0.6355078220367432\n",
      "Epoch [8/15], Loss: 0.554839015007019\n",
      "Epoch [9/15], Loss: 0.5717707872390747\n",
      "Epoch [10/15], Loss: 0.6000287532806396\n",
      "Epoch [11/15], Loss: 0.5802218317985535\n",
      "Epoch [12/15], Loss: 0.643696665763855\n",
      "Epoch [13/15], Loss: 0.6242877244949341\n",
      "Epoch [14/15], Loss: 0.6028377413749695\n",
      "Epoch [15/15], Loss: 0.6072602868080139\n",
      "Accuracy: 0.7782240316901409\n",
      "Model 64 finished\n",
      "Best Hyperparameters: {'num_layers': 1, 'hidden_size': 10, 'learning_rate': 0.01, 'dropout_rate': 0.2, 'batch_size': 64, 'num_epochs': 15}\n",
      "Best Accuracy: 0.7849911971830986\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameter optimization\n",
    "\n",
    "hyperparameters = {\n",
    "    'num_layers': [1, 2, 3, 4],\n",
    "    'hidden_size': [10, 25],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "    'batch_size': [64],\n",
    "    'num_epochs': [15]\n",
    "}\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_hyperparameters = {}\n",
    "\n",
    "# Best subset selection\n",
    "model = 1\n",
    "for num_layers in hyperparameters['num_layers']:\n",
    "    for hidden_size in hyperparameters['hidden_size']:\n",
    "        for learning_rate in hyperparameters['learning_rate']:\n",
    "            for dropout_rate in hyperparameters['dropout_rate']:\n",
    "                for batch_size in hyperparameters['batch_size']:\n",
    "                    for num_epochs in hyperparameters['num_epochs']:\n",
    "\n",
    "                        # Optimizer\n",
    "                        opt = optim.Adam(clf.parameters(), lr=learning_rate)\n",
    "                        # Loss function\n",
    "                        loss_fn = nn.BCELoss()\n",
    "\n",
    "                        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "                        # Make model\n",
    "                        clf = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout_rate=dropout_rate)\n",
    "                        \n",
    "                        # Train\n",
    "                        for epoch in range(num_epochs):\n",
    "                            batch = 0\n",
    "                            for data, label in train_loader:\n",
    "                                \n",
    "                                # Predictions\n",
    "                                yhat = clf(data).squeeze(dim=1)\n",
    "                                # Calculate loss\n",
    "                                loss = loss_fn(yhat, label.float())\n",
    "\n",
    "                                # Back propagation\n",
    "                                opt.zero_grad()\n",
    "                                loss.backward()\n",
    "                                opt.step()\n",
    "\n",
    "                            # Epoch performance\n",
    "                            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss}')\n",
    "\n",
    "\n",
    "                        # Testing\n",
    "                        y_test_tens = torch.LongTensor(y_test)\n",
    "                        test_dataset = TensorDataset(x_test, y_test_tens)\n",
    "                        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "                        # Don't train the model any more\n",
    "                        clf.eval()\n",
    "\n",
    "                        total_samples = 0\n",
    "                        correct_predictions = 0\n",
    "                        for batch in test_loader:\n",
    "                            out = clf(batch[0]).squeeze(dim=1)\n",
    "\n",
    "                            out = out.ge(0.5)\n",
    "                            out = out.int().reshape(1, -1)\n",
    "\n",
    "                            # Update counts\n",
    "                            correct_predictions += torch.sum(out == batch[1]).item()\n",
    "                            total_samples += batch[1].size(0)\n",
    "                        accuracy = correct_predictions / total_samples\n",
    "                        print(f'Accuracy: {accuracy}')\n",
    "\n",
    "                        # Update the best possible subset if needed\n",
    "                        if accuracy > best_accuracy:\n",
    "                            best_accuracy = accuracy\n",
    "                            best_hyperparameters = {\n",
    "                                'num_layers': num_layers,\n",
    "                                'hidden_size': hidden_size,\n",
    "                                'learning_rate': learning_rate,\n",
    "                                'dropout_rate': dropout_rate,\n",
    "                                'batch_size': batch_size,\n",
    "                                'num_epochs': num_epochs\n",
    "                            }\n",
    "                        # Keep track of where we are\n",
    "                        # Because it takes like an hour\n",
    "                        print(f'Model {model} finished')\n",
    "                        model += 1\n",
    "# print the results\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 100, Loss: 0.40653443336486816\n",
      "Epoch: 0, Batch: 200, Loss: 0.3550562262535095\n",
      "Epoch: 0, Batch: 300, Loss: 0.3551463186740875\n",
      "Epoch: 0, Batch: 400, Loss: 0.24728640913963318\n",
      "Epoch: 0, Batch: 500, Loss: 0.35005974769592285\n",
      "Epoch: 0, Batch: 600, Loss: 0.26467257738113403\n",
      "Epoch [1/15], Loss: 0.6018450856208801\n",
      "Epoch: 1, Batch: 100, Loss: 0.417006254196167\n",
      "Epoch: 1, Batch: 200, Loss: 0.15704463422298431\n",
      "Epoch: 1, Batch: 300, Loss: 0.33455419540405273\n",
      "Epoch: 1, Batch: 400, Loss: 0.24234077334403992\n",
      "Epoch: 1, Batch: 500, Loss: 0.37085339426994324\n",
      "Epoch: 1, Batch: 600, Loss: 0.28023743629455566\n",
      "Epoch [2/15], Loss: 0.492572546005249\n",
      "Epoch: 2, Batch: 100, Loss: 0.3613828122615814\n",
      "Epoch: 2, Batch: 200, Loss: 0.4198001027107239\n",
      "Epoch: 2, Batch: 300, Loss: 0.42834344506263733\n",
      "Epoch: 2, Batch: 400, Loss: 0.35202330350875854\n",
      "Epoch: 2, Batch: 500, Loss: 0.340670645236969\n",
      "Epoch: 2, Batch: 600, Loss: 0.2357664853334427\n",
      "Epoch [3/15], Loss: 0.3307550251483917\n",
      "Epoch: 3, Batch: 100, Loss: 0.26582449674606323\n",
      "Epoch: 3, Batch: 200, Loss: 0.42750513553619385\n",
      "Epoch: 3, Batch: 300, Loss: 0.20241856575012207\n",
      "Epoch: 3, Batch: 400, Loss: 0.32629361748695374\n",
      "Epoch: 3, Batch: 500, Loss: 0.2077464461326599\n",
      "Epoch: 3, Batch: 600, Loss: 0.5625114440917969\n",
      "Epoch [4/15], Loss: 0.34348589181900024\n",
      "Epoch: 4, Batch: 100, Loss: 0.38295143842697144\n",
      "Epoch: 4, Batch: 200, Loss: 0.37450850009918213\n",
      "Epoch: 4, Batch: 300, Loss: 0.3775472640991211\n",
      "Epoch: 4, Batch: 400, Loss: 0.2698216140270233\n",
      "Epoch: 4, Batch: 500, Loss: 0.38865846395492554\n",
      "Epoch: 4, Batch: 600, Loss: 0.21671317517757416\n",
      "Epoch [5/15], Loss: 0.3161357641220093\n",
      "Epoch: 5, Batch: 100, Loss: 0.31008824706077576\n",
      "Epoch: 5, Batch: 200, Loss: 0.32750993967056274\n",
      "Epoch: 5, Batch: 300, Loss: 0.3865712881088257\n",
      "Epoch: 5, Batch: 400, Loss: 0.4021401107311249\n",
      "Epoch: 5, Batch: 500, Loss: 0.3630625903606415\n",
      "Epoch: 5, Batch: 600, Loss: 0.2680995762348175\n",
      "Epoch [6/15], Loss: 0.2525614798069\n",
      "Epoch: 6, Batch: 100, Loss: 0.32767927646636963\n",
      "Epoch: 6, Batch: 200, Loss: 0.3380338251590729\n",
      "Epoch: 6, Batch: 300, Loss: 0.3034849464893341\n",
      "Epoch: 6, Batch: 400, Loss: 0.366926908493042\n",
      "Epoch: 6, Batch: 500, Loss: 0.3004344403743744\n",
      "Epoch: 6, Batch: 600, Loss: 0.2594973146915436\n",
      "Epoch [7/15], Loss: 0.2621367573738098\n",
      "Epoch: 7, Batch: 100, Loss: 0.30279821157455444\n",
      "Epoch: 7, Batch: 200, Loss: 0.2301388680934906\n",
      "Epoch: 7, Batch: 300, Loss: 0.40963736176490784\n",
      "Epoch: 7, Batch: 400, Loss: 0.3282186985015869\n",
      "Epoch: 7, Batch: 500, Loss: 0.19917570054531097\n",
      "Epoch: 7, Batch: 600, Loss: 0.2231767624616623\n",
      "Epoch [8/15], Loss: 0.48010173439979553\n",
      "Epoch: 8, Batch: 100, Loss: 0.4140034019947052\n",
      "Epoch: 8, Batch: 200, Loss: 0.3884832561016083\n",
      "Epoch: 8, Batch: 300, Loss: 0.33208316564559937\n",
      "Epoch: 8, Batch: 400, Loss: 0.2775888741016388\n",
      "Epoch: 8, Batch: 500, Loss: 0.265159010887146\n",
      "Epoch: 8, Batch: 600, Loss: 0.40651944279670715\n",
      "Epoch [9/15], Loss: 0.3926529288291931\n",
      "Epoch: 9, Batch: 100, Loss: 0.3015710413455963\n",
      "Epoch: 9, Batch: 200, Loss: 0.29120922088623047\n",
      "Epoch: 9, Batch: 300, Loss: 0.29661768674850464\n",
      "Epoch: 9, Batch: 400, Loss: 0.35511907935142517\n",
      "Epoch: 9, Batch: 500, Loss: 0.24110886454582214\n",
      "Epoch: 9, Batch: 600, Loss: 0.35443517565727234\n",
      "Epoch [10/15], Loss: 0.45358890295028687\n",
      "Epoch: 10, Batch: 100, Loss: 0.40803003311157227\n",
      "Epoch: 10, Batch: 200, Loss: 0.36177772283554077\n",
      "Epoch: 10, Batch: 300, Loss: 0.33393165469169617\n",
      "Epoch: 10, Batch: 400, Loss: 0.27359503507614136\n",
      "Epoch: 10, Batch: 500, Loss: 0.30942732095718384\n",
      "Epoch: 10, Batch: 600, Loss: 0.3597261607646942\n",
      "Epoch [11/15], Loss: 0.3413143754005432\n",
      "Epoch: 11, Batch: 100, Loss: 0.37839728593826294\n",
      "Epoch: 11, Batch: 200, Loss: 0.4036571681499481\n",
      "Epoch: 11, Batch: 300, Loss: 0.2750453054904938\n",
      "Epoch: 11, Batch: 400, Loss: 0.2596093714237213\n",
      "Epoch: 11, Batch: 500, Loss: 0.24100187420845032\n",
      "Epoch: 11, Batch: 600, Loss: 0.244085431098938\n",
      "Epoch [12/15], Loss: 0.3621610105037689\n",
      "Epoch: 12, Batch: 100, Loss: 0.4200665354728699\n",
      "Epoch: 12, Batch: 200, Loss: 0.3838880956172943\n",
      "Epoch: 12, Batch: 300, Loss: 0.29018115997314453\n",
      "Epoch: 12, Batch: 400, Loss: 0.2960682213306427\n",
      "Epoch: 12, Batch: 500, Loss: 0.5194755792617798\n",
      "Epoch: 12, Batch: 600, Loss: 0.36657601594924927\n",
      "Epoch [13/15], Loss: 0.38354191184043884\n",
      "Epoch: 13, Batch: 100, Loss: 0.2980719208717346\n",
      "Epoch: 13, Batch: 200, Loss: 0.3586031496524811\n",
      "Epoch: 13, Batch: 300, Loss: 0.34899458289146423\n",
      "Epoch: 13, Batch: 400, Loss: 0.2963983118534088\n",
      "Epoch: 13, Batch: 500, Loss: 0.4275946319103241\n",
      "Epoch: 13, Batch: 600, Loss: 0.2868184447288513\n",
      "Epoch [14/15], Loss: 0.4209359884262085\n",
      "Epoch: 14, Batch: 100, Loss: 0.36748743057250977\n",
      "Epoch: 14, Batch: 200, Loss: 0.24410656094551086\n",
      "Epoch: 14, Batch: 300, Loss: 0.36917251348495483\n",
      "Epoch: 14, Batch: 400, Loss: 0.2847915291786194\n",
      "Epoch: 14, Batch: 500, Loss: 0.2865722179412842\n",
      "Epoch: 14, Batch: 600, Loss: 0.44559013843536377\n",
      "Epoch [15/15], Loss: 0.3043406009674072\n"
     ]
    }
   ],
   "source": [
    "# Using the best set of hyperparameters\n",
    "clf = LSTMModel(input_size, best_hyperparameters['hidden_size'], best_hyperparameters['num_layers'], output_size, best_hyperparameters['dropout_rate'])\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "opt = optim.Adam(clf.parameters(), lr=best_hyperparameters['learning_rate'])\n",
    "# Loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "for epoch in range(best_hyperparameters['num_epochs']):\n",
    "    batch = 0\n",
    "    for data, label in train_loader:\n",
    "        \n",
    "        yhat = clf(data).squeeze(dim=1)\n",
    "\n",
    "        loss = loss_fn(yhat, label.float())\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        batch += 1\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch}, Loss: {loss}')\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{15}], Loss: {loss}')\n",
    "\n",
    "# Testing\n",
    "y_test_tens = torch.LongTensor(y_test)\n",
    "test_dataset = TensorDataset(x_test, y_test_tens)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_hyperparameters['batch_size'], shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for the model:\n",
      "Percision score: 0.22776183644189382\n",
      "Accuracy score: 0.6947073063380281\n",
      "Recall: 0.15752914909451748\n",
      "F1 score: 0.18624431734858485\n",
      "Matrix: [[11992  2153]\n",
      " [ 3396   635]]\n"
     ]
    }
   ],
   "source": [
    "# Don't train the model any more\n",
    "clf.eval()\n",
    "\n",
    "# Do the testing\n",
    "predictions = []\n",
    "for batch in test_loader:\n",
    "    out = clf(batch[0]).squeeze(dim=1)\n",
    "\n",
    "    out = out.ge(0.5)\n",
    "    out = out.int().reshape(1, -1)\n",
    "\n",
    "    # Update counts\n",
    "    correct_predictions += torch.sum(out == batch[1]).item()\n",
    "    total_samples += batch[1].size(0)\n",
    "\n",
    "    predictions.append(out.reshape(-1, 1))\n",
    "    \n",
    "# Show the stats for the model\n",
    "all_pred = torch.cat(predictions, dim=0).squeeze(1)\n",
    "print(f\"Stats for the model:\\nPercision score: {metrics.precision_score(y_test_tens, all_pred)}\\nAccuracy score: {metrics.accuracy_score(y_test_tens, all_pred)}\\nRecall: {metrics.recall_score(y_test_tens, all_pred)}\\nF1 score: {metrics.f1_score(y_test_tens, all_pred)}\\nMatrix: {metrics.confusion_matrix(y_test_tens, all_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Results </b> <br>\n",
    "The results of the hyperparameter optimization don't make sense. I think there is something underlying that I cannot see that is causing my training set to change slightly between runs giving a far below accuracy. Using some of the parameters I have given before hand, I have managed to get ~86% accuracy on a good run. This is later discussed in the analysis where I compare it to the fully connected neural network from the previous assignment. Despite this, I am not quite sure why the accuracy of the model is going down slightly overtime. I have tested this, as the accuracy of the model before the hyperparameters is ~85%, but will dip to ~68% after running some other models. However, we can see that using the best hyperparameters the results are: <br>\n",
    "Stats for the model: <br>\n",
    "Percision score: 0.213640610401744 <br>\n",
    "Accuracy score: 0.6770466549295775 <br>\n",
    "Recall: 0.17018109650210866 <br>\n",
    "F1 score: 0.18945042805854737 <br>\n",
    "Matrix: [[11620  2525] <br>\n",
    "[ 3345   686]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Analysis </b> <br>\n",
    "#### Methodologies <br>\n",
    "Using the data from the previous fully connected neural network as the data for the LSTM, we can make a comparison in how they compare to one another with no outside changes to the underlying data. As stated, I opted to use an LSTM over GRU due to not having the constraints that would make and LSTM harder to get away with. Despite this, the model for my LSTM is still quite simple containing the LSTM layer, a normalization layer, a dropout layer, a fully connected layer (with ReLU) (FCNN), a linear layer, followed by a sigmoid for the output layer. As well as, adam for the optimizer and Binary Cross Entropy for the loss function. <br>\n",
    "#### Model performance <br>\n",
    "From some set hyperparameters before any tuning what-so-ever, I managed to get the same accuracy as the FCNN with my LSTM. However, after optimizing the hyperparameters, the accuracy is far lower than expected for some reason as mentioned. However, if this bug was to be fixed I think I could achieve accuracy of ~86%-88% if the hyperparameters were found. <br>\n",
    "\n",
    "#### Comparison <br>\n",
    "Comparing the fully connected neural network, from previous assignment, to the LSTM here, the results are almost the same. Averaging around the 85% accuracy mark for predictions. While both models did a good job in the end with being able to choose correctly most of the time, one is better than the other. When it came to the training of the models, the FCNN was far more computationaly simple. This came into affect when attempting to find the best hyperparameters. The LSTM took a while at over an hour to run the best possible subset selection for the given ones. Whereas, the FCNN took minutes. <br>\n",
    "#### Personal take <br>\n",
    "Despite this, I really had to wonder at the end, why are the two models so similar in their accuracy? I can't exactly pinpoint why, but I would say it relates to what I discussed in the previous assignment where my data is a bit more sloppy in areas and I didn't take as much time as I should have. This likely lead to a getting an almost equal prediction rate. <br>\n",
    "#### Reflection on this project <br>\n",
    "I have never worked on any topics within machine learning prior to this class. And trying to take in everything throughout this semester and shove in some of the later topics into this project really pushed me to the edge. I think learning how to work with pytorch will be invaluable in the long run where I want to explore it on my own. Even though it sounds like this may be bad, personally, I do need to be pushed to that edge and basically perform a learn it or fail way. I have my doubts in being able to understand what I've done in this project, but within a few months from now I will have come out better knowing what I have done and be able to expand upon what I have done. <br>\n",
    "#### Pytorch specific reflection <br>\n",
    "Honestly, there's not that much to talk about when it comes to pytorch, I've learned a little more about how tensors work, but the concept still makes me dizzy. The actual model creation is simple for someone who doesn't need to go into it. So as long as I know roughly what everything does and how it can relate to the data at hand I can more or less just shove things in and likely get what I want out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
